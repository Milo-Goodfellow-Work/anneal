#!/usr/bin/env python3
from __future__ import annotations

# =========================================================================================
# ANNEAL: UNIVERSAL VERIFICATION AGENT
# =========================================================================================
# This toolchain is designed to translate AND verify programs from ANY source language
# (C, C++, Rust, JavaScript, Python, Go, etc.) into Lean 4 for formal verification.
#
# While the current demo uses C ('order_engine'), the architecture is language-agnostic.
# The 'harness' and 'differential testing' components are abstract interfaces that
# can wrap any language runtime. DO NOT assume C-specific compilation steps (like 'gcc')
# are eventually this will dispatch to language-specific drivers.
# =========================================================================================

import os
import re
import sys
import time
import json
import shutil
import signal
import shlex
import tomllib
import asyncio
import subprocess
from pathlib import Path
from typing import List, Optional, Dict, Tuple, Any, Set
from dataclasses import dataclass

from openai import OpenAI

try:
    import aristotlelib
except ImportError:
    aristotlelib = None


# ============================================================
# Configuration
# ============================================================

SECRETS_FILE = Path("secrets.toml")

SPEC_DIR = Path("spec").resolve()
SPEC_SRC_DIR = SPEC_DIR / "Spec"
EXAMPLES_DIR = Path("examples").resolve()

MODEL_ID = "gpt-5.2"

PRINT_TRUNC = 4000
MAX_TOOL_READ_CHARS = 80_000

# "Must converge" philosophy:
# - we keep trying for a long time and we do NOT proceed to later stages if we fail.
MAX_REPAIR_TURNS_PER_FILE = 80
MAX_REPAIR_TURNS_GLOBAL = 120
MAX_SESSION_TURNS = 16

# Autogenerated, model-writable artifacts outside Spec/Spec
SPEC_TESTS_DIR = SPEC_DIR / "tests"
SPEC_REPORTS_DIR = SPEC_DIR / "reports"

# Files that aggregate imports / define the environment surface.
# We auto-generate and keep them read-only for the model.
LOCKED_LEAN_FILENAMES = {"Prelude.lean"}  # plus per-project root module "<project>.lean" added dynamically

# Prelude policy: we own these imports; generated files should only import Spec.Prelude.
PRELUDE_REQUIRED_IMPORTS = [
    "Std",
    "Std.Data.TreeMap",
    "Std.Data.TreeSet",
    "Std.Data.HashMap",
    "Std.Data.HashSet",
]

# Differential testing robustness requirements
DIFF_REQUIRED_RUNS = 5          # number of distinct seeds / runs required to pass before equivalence can complete
DIFF_MIN_CASES_PER_RUN = 5      # minimum number of command lines / cases per run
DIFF_SEED_START = 1             # seeds will be DIFF_SEED_START..DIFF_SEED_START+DIFF_REQUIRED_RUNS-1

# Subprocess timeouts
GEN_TIMEOUT_S = 8
C_RUN_TIMEOUT_S = 8
LEAN_RUN_TIMEOUT_S = 3000  # increase to avoid "fake completion" on 10s timeouts

# Anti-laziness / safety-critical enforcement
# We forbid these phrases in translated Lean code (not in Verif.lean).
SKIP_TO_EQUIVALENCE = True
SKIP_TO_HARDENING = False
SKIP_TO_SPECIFICATION = False  # Skip Translation and Equivalence, go straight to Specification
SKIP_TO_VERIFICATION = False

FORBIDDEN_TRANSLATION_PHRASES = [
    "for this benchmark",
    "as a benchmark",
    "placeholder",
    "stub",
    "not implemented",
    "todo:",
    "TODO",
    "a full translation would",
    "we return `true` as a placeholder",
    "return `true` as a placeholder",
    "return true as a placeholder",
    "not going to fully complete",
    "not going to actually fully complete",
    "for the purpose of this benchmark",
    "for the purposes of this benchmark",
    "simplified for benchmark",
]

# If a function name suggests safety checking, ban constant-true bodies.
SUSPICIOUS_SAFETY_FN_TOKENS = [
    "verify",
    "integrity",
    "check",
    "invariant",
    "validate",
    "wellformed",
    "well_formed",
]


# ============================================================
# Utilities
# ============================================================

def log(msg: str) -> None:
    print(f"[Anneal] {msg}", flush=True)

def trunc(s: str, n: int = PRINT_TRUNC) -> str:
    if s is None:
        return ""
    return s if len(s) <= n else (s[:n] + f"\n... (truncated, total {len(s)} chars)")

def trunc_tail(s: str, n: int = PRINT_TRUNC) -> str:
    """Truncate from the BEGINNING, keeping the END of the string.
    
    Useful for build output where warnings come first and actual errors come last.
    Lean's lake build puts errors at the end, so we need to preserve those.
    """
    if s is None:
        return ""
    if len(s) <= n:
        return s
    return f"(... truncated {len(s) - n} chars from start ...)\n" + s[-(n):]

def load_secrets() -> dict:
    if not SECRETS_FILE.exists():
        key = os.environ.get("OPENAI_API_KEY")
        if key:
            aristotle_key = os.environ.get("ARISTOTLE_API_KEY")
            secrets = {"OPENAI_API_KEY": key}
            if aristotle_key:
                secrets["ARISTOTLE_API_KEY"] = aristotle_key
            return {"secrets": secrets}
        raise FileNotFoundError(f"{SECRETS_FILE} not found and OPENAI_API_KEY not in env.")
    with SECRETS_FILE.open("rb") as f:
        return tomllib.load(f)

def _safe_relpath(p: str) -> str:
    p = (p or "").replace("\\", "/").lstrip("/")
    if p == "" or p == ".":
        raise ValueError("Empty path")
    parts = [x for x in p.split("/") if x not in ("", ".")]
    if any(x == ".." for x in parts):
        raise ValueError(f"Path traversal not allowed: {p}")
    return "/".join(parts)

def _read_text_file(p: Path) -> str:
    return p.read_text(encoding="utf-8", errors="replace")

def _write_text_file(p: Path, content: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(content, encoding="utf-8")

def list_project_files(base_dir: Path) -> List[str]:
    files: List[str] = []
    if not base_dir.exists():
        return files
    for root, dirs, filenames in os.walk(base_dir):
        if ".git" in dirs:
            dirs.remove(".git")
        if "__pycache__" in dirs:
            dirs.remove("__pycache__")
        for fn in filenames:
            abs_p = Path(root) / fn
            rel_p = abs_p.relative_to(base_dir)
            files.append(str(rel_p).replace("\\", "/"))
    return sorted(files)

def list_lean_files(base_dir: Path) -> List[str]:
    files: List[str] = []
    if not base_dir.exists():
        return files
    for root, dirs, filenames in os.walk(base_dir):
        if ".git" in dirs:
            dirs.remove(".git")
        if "__pycache__" in dirs:
            dirs.remove("__pycache__")
        for fn in filenames:
            if fn.endswith(".lean"):
                abs_p = Path(root) / fn
                rel_p = abs_p.relative_to(base_dir)
                files.append(str(rel_p).replace("\\", "/"))
    return sorted(files)

def run_lake_build(cwd: Path) -> str:
    start = time.time()
    try:
        res = subprocess.run(
            ["lake", "build"],
            cwd=str(cwd),
            capture_output=True,
            text=True,
            check=False,
        )
        elapsed = time.time() - start
        if res.returncode == 0:
            return f"Build Success ({elapsed:.2f}s)"
        return f"Build Failed (exit={res.returncode}, {elapsed:.2f}s):\n{res.stderr}\n{res.stdout}"
    except Exception as e:
        return f"Error running lake build: {e}"

def run_lake_build_target(cwd: Path, target: Optional[str] = None) -> str:
    start = time.time()
    cmd = ["lake", "build"]
    if target:
        cmd.append(target)
    try:
        res = subprocess.run(cmd, cwd=str(cwd), capture_output=True, text=True, check=False)
        elapsed = time.time() - start
        if res.returncode == 0:
            return f"Build Success ({elapsed:.2f}s)"
        return f"Build Failed (exit={res.returncode}, {elapsed:.2f}s):\n{res.stderr}\n{res.stdout}"
    except Exception as e:
        return f"Error running lake build: {e}"

def module_name_from_lean_path(rel_path_under_spec: str) -> Optional[str]:
    p = Path(rel_path_under_spec)
    if p.suffix != ".lean":
        return None
    return "Spec." + ".".join(p.with_suffix("").parts)

def _slug_to_camel(s: str) -> str:
    parts = re.split(r"[^A-Za-z0-9]+", s)
    parts = [x for x in parts if x]
    if not parts:
        return "X"
    out = "".join(x[:1].upper() + x[1:] for x in parts)
    if out and out[0].isdigit():
        out = "X" + out
    return out

def _lean_out_path_for_source(project: str, src_rel: str, used: Dict[str, int]) -> str:
    src = Path(src_rel)
    stem = src.stem
    parent = str(src.parent).replace("\\", "/")
    if parent in (".", ""):
        base = _slug_to_camel(stem)
    else:
        base = _slug_to_camel(parent.replace("/", "_") + "_" + stem)
    name = base
    if name in used:
        used[name] += 1
        name = f"{name}{used[name]}"
    else:
        used[name] = 1
    return f"{project}/{name}.lean"

def _is_source_file(rel: str) -> bool:
    ext = Path(rel).suffix.lower()
    return ext in {".c", ".h", ".cc", ".cpp", ".hpp"}

def _limit_lines(lines: List[str], max_lines: int = 220) -> List[str]:
    if len(lines) <= max_lines:
        return lines
    return lines[:max_lines] + [f"... ({len(lines)-max_lines} more lines omitted) ..."]


# ============================================================
# Lean error parsing (for targeted repair)
# ============================================================

LEAN_ERR_RE = re.compile(
    r"^error:\s+(?P<file>[^:]+):(?P<line>\d+):(?P<col>\d+):\s+(?P<msg>.*)$",
    re.MULTILINE,
)

@dataclass
class LeanError:
    file: str
    line: int
    col: int
    msg: str

def parse_lean_errors(build_output: str, *, max_n: int = 6) -> List[LeanError]:
    errs: List[LeanError] = []
    for m in LEAN_ERR_RE.finditer(build_output):
        errs.append(
            LeanError(
                file=m.group("file").strip(),
                line=int(m.group("line")),
                col=int(m.group("col")),
                msg=m.group("msg").strip(),
            )
        )
        if len(errs) >= max_n:
            break
    return errs

def excerpt_around(text: str, line_1based: int, *, radius: int = 14) -> str:
    lines = text.splitlines()
    i = max(1, line_1based) - 1
    lo = max(0, i - radius)
    hi = min(len(lines), i + radius + 1)
    out: List[str] = []
    for idx in range(lo, hi):
        prefix = ">>" if idx == i else "  "
        out.append(f"{prefix} {idx+1:4d}: {lines[idx]}")
    return "\n".join(out)


# ============================================================
# Prelude + locked files
# ============================================================

PRELUDE_PATH = SPEC_SRC_DIR / "Prelude.lean"

def ensure_prelude_and_lockdown() -> None:
    """
    Ensure Prelude exists and contains a conservative, useful import surface.
    The model is prevented from writing it; the script owns it.
    """
    SPEC_SRC_DIR.mkdir(parents=True, exist_ok=True)

    required_import_lines = [f"import {m}" for m in PRELUDE_REQUIRED_IMPORTS]

    if not PRELUDE_PATH.exists():
        content = (
            "\n".join(required_import_lines)
            + "\n\nnamespace Spec\n\n"
              "-- Shared abbreviations and safe utilities live here.\n"
              "-- Generated modules must import ONLY Spec.Prelude (plus Spec.<project>.* if needed).\n\n"
              "abbrev U8  := UInt8\n"
              "abbrev U16 := UInt16\n"
              "abbrev U32 := UInt32\n"
              "abbrev U64 := UInt64\n\n"
              "end Spec\n"
        )
        _write_text_file(PRELUDE_PATH, content)
        log(f"Wrote {PRELUDE_PATH}")
        return

    existing = _read_text_file(PRELUDE_PATH)
    existing_imports = set(re.findall(r"^\s*import\s+([A-Za-z0-9_.]+)\s*$", existing, flags=re.MULTILINE))
    missing = [m for m in PRELUDE_REQUIRED_IMPORTS if m not in existing_imports]
    if missing:
        prepend = "\n".join([f"import {m}" for m in missing]) + "\n"
        _write_text_file(PRELUDE_PATH, prepend + existing)
        log(f"Updated {PRELUDE_PATH}: prepended missing imports: {missing}")


# ============================================================
# Tool schema
# ============================================================

def _tool(name: str, description: str, properties: Dict[str, Any], required: List[str]) -> Dict[str, Any]:
    return {
        "type": "function",
        "name": name,
        "description": description,
        "strict": True,
        "parameters": {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False,
        },
    }

TOOLS_SCHEMA = [
    _tool(
        "read_source_file",
        "Read content of a source file from source project (examples/<project>/).",
        {"path": {"type": "string", "description": "Relative path under examples/<project>/"}},
        ["path"],
    ),
    _tool(
        "read_lean_file",
        "Read content of a Lean file from spec/Spec/ (relative path).",
        {"path": {"type": "string", "description": "Relative path under spec/Spec/ (e.g., 'order_engine/Engine.lean')"}},
        ["path"],
    ),
    _tool(
        "write_lean_file",
        "Write or overwrite a Lean file under spec/Spec/ (relative path). Writes are restricted to pre-autogenerated files only.",
        {
            "path": {"type": "string", "description": "Relative path under spec/Spec/"},
            "content": {"type": "string", "description": "Full content of the Lean file"},
        },
        ["path", "content"],
    ),
    _tool(
        "write_text_file",
        "Write or overwrite a non-Lean file. Writes are restricted to pre-autogenerated files only.",
        {
            "path": {"type": "string", "description": "Relative path under the repository root (e.g., 'spec/tests/harness.c')"},
            "content": {"type": "string", "description": "Full content of the file"},
        },
        ["path", "content"],
    ),
    _tool(
        "verify_build",
        "Run lake build in the spec/ directory.",
        {},
        [],
    ),
    _tool(
        "restart_translation",
        "CRITICAL: Call this only if you determine that the current translation is fundamentally flawed and cannot be fixed by editing files. This will discard current progress and restart the Translation stage with your feedback.",
        {
            "reason": {"type": "string", "description": "Detailed explanation of why the semantic mismatch is unfixable and requires re-translation."}
        },
        ["reason"],
    ),
    _tool(
        "run_differential_test",
        "Run robust differential tests (multiple seeds) between C harness and Lean harness; returns JSON status.",
        {
            "gen_script_path": {"type": "string", "description": "Path to python input generator (relative to repo root)"},
            "c_harness_path": {"type": "string", "description": "Path to C harness source (relative to repo root)"},
            "lean_harness_path": {"type": "string", "description": "Path to Lean harness source (relative to spec/Spec/)"},
        },
        ["gen_script_path", "c_harness_path", "lean_harness_path"],
    ),
    _tool(
        "submit_stage",
        "Signal that the current stage is complete (this may be denied if success criteria are not met).",
        {"summary": {"type": "string", "description": "Brief summary"}},
        ["summary"],
    ),
]


# ============================================================
# Import policy + content validators
# ============================================================

IMPORT_RE = re.compile(r"^\s*import\s+([A-Za-z0-9_.]+)\s*$", re.MULTILINE)

# Verif.lean anti-smuggling policy:
# - Verif.lean may contain theorems/lemmas (and supporting proof code), but must not define executable entities.
# - This prevents runtime behavior from being "smuggled" through Verif.lean (which is imported for Aristotle).
VERIF_FORBIDDEN_TOPLEVEL_RE = re.compile(
    r"^\s*(?:@\[[^\]]*\]\s*)*(?:(?:private|protected|noncomputable|unsafe|partial)\s+)*"
    r"(def|abbrev|instance|structure|inductive|axiom|constant|macro|syntax|notation|attribute)\b",
    re.MULTILINE,
)

def validate_imports_for_project(project: str, content: str) -> Tuple[bool, List[str]]:
    """
    Policy:
    - Must import Spec.Prelude at least once.
    - May additionally import Spec.<project>.* (rarely needed).
    - No Std / Mathlib / anything else in generated files.
    """
    mods = IMPORT_RE.findall(content)
    bad: List[str] = []
    for mod in mods:
        if mod == "Spec.Prelude":
            continue
        if mod.startswith(f"Spec.{project}."):
            continue
        if mod == f"Spec.{project}":
            continue
        bad.append(mod)

    if "Spec.Prelude" not in mods:
        bad.append("(missing Spec.Prelude)")

    return (len(bad) == 0, bad)

IMPORT_FAILED_RE = re.compile(r"import\s+(?P<mod>[A-Za-z0-9_.]+)\s+failed")
ENV_CONTAINS_FROM_RE = re.compile(
    r"environment already contains '[^']+'\s+from\s+(?P<mod>[A-Za-z0-9_.]+)"
)

def module_to_spec_relpath(mod: str) -> Optional[str]:
    # Spec.order_engine.Engine2 -> order_engine/Engine2.lean
    if not mod.startswith("Spec."):
        return None
    parts = mod.split(".")[1:]  # drop leading "Spec"
    if not parts:
        return None
    return "/".join(parts) + ".lean"

def extract_failed_import_module(msg: str) -> Optional[str]:
    m = IMPORT_FAILED_RE.search(msg or "")
    return m.group("mod") if m else None

def _contains_forbidden_phrases(s: str) -> Optional[str]:
    lo = s.lower()
    for ph in FORBIDDEN_TRANSLATION_PHRASES:
        if ph.lower() in lo:
            return ph
    return None

def _looks_like_constant_true_safety_check(lean: str) -> bool:
    """
    Heuristic: if there is a def whose name suggests "verify/check/integrity" and its body is literally `true`,
    reject it (outside Verif.lean).
    """
    for m in re.finditer(r"^\s*(?:partial\s+)?def\s+([A-Za-z0-9_']+)[^:=\n]*:=\s*true\s*$", lean, flags=re.MULTILINE):
        name = m.group(1).lower()
        if any(tok in name for tok in SUSPICIOUS_SAFETY_FN_TOKENS):
            return True
    return False

def validate_basic_lean_shape(project: str, rel_path: str, content: str) -> Tuple[bool, str]:
    stripped = content.strip()
    if not stripped:
        return False, "File content is empty or whitespace."

    if "namespace" not in content or f"namespace Spec.{project}" not in content:
        return False, f"Missing required namespace 'namespace Spec.{project}'."

    if f"end Spec.{project}" not in content:
        return False, f"Missing required end marker 'end Spec.{project}'."

    ok_imports, bad = validate_imports_for_project(project, content)
    if not ok_imports:
        return False, f"Import policy violated: {bad}. Only 'import Spec.Prelude' (and optionally Spec.{project}.*) allowed."

    is_verif = rel_path.replace("\\", "/").endswith("/Verif.lean")
    if is_verif:
        # Proofs-only: no executable definitions or axioms/macros/etc.
        if VERIF_FORBIDDEN_TOPLEVEL_RE.search(content):
            return (
                False,
                "Verif.lean is proofs-only (anti-smuggling): it must not contain def/abbrev/instance/structure/"
                "inductive/axiom/constant/macro/syntax/notation/attribute.",
            )
        # 'sorry' is allowed here by design.
        return True, ""

    # Non-Verif files: strict anti-laziness and anti-placeholder rules.
    if re.search(r"\bsorry\b", content):
        return False, "Forbidden: 'sorry' is not allowed in translated/runtime Lean modules (only allowed in Verif.lean)."

    bad_phrase = _contains_forbidden_phrases(content)
    if bad_phrase:
        return False, f"Forbidden phrase detected in translated code: '{bad_phrase}'. Safety-critical mode forbids placeholders/benchmark language."

    if _looks_like_constant_true_safety_check(content):
        return False, "Forbidden: safety-checking function appears to be a constant `true`. Implement the real check."

    return True, ""


# ============================================================
# Processor
# ============================================================

class RestartTranslationError(Exception):
    """Raised when the model decides the translation is fundamentally flawed and needs a fresh start."""
    def __init__(self, reason: str):
        self.reason = reason
        super().__init__(reason)

class ProjectProcessor:
    def __init__(self, example_name: str, example_path: Path, client: OpenAI, secrets: dict):
        self.name = example_name
        self.source_root = example_path

        self.spec_pkg_root = SPEC_DIR
        self.spec_src_root = SPEC_SRC_DIR
        self.spec_project_root = self.spec_src_root / example_name  # spec/Spec/<project>/

        self.client = client
        self.secrets = secrets

        self.spec_project_root.mkdir(parents=True, exist_ok=True)

        # Autogenerated write-allowlists
        self.allowed_lean_writes: Set[str] = set()   # paths relative to spec/Spec
        self.allowed_text_writes: Set[str] = set()   # paths relative to repo root
        self.locked_lean_paths: Set[str] = set(LOCKED_LEAN_FILENAMES)

        # Source->Lean mapping
        self.src_to_lean: Dict[str, str] = {}
        self.lean_to_src: Dict[str, str] = {}

        # Stage tracking + gating
        self._current_stage: str = "INIT"
        self._equiv_state: Dict[str, Any] = {
            "last_report": None,
            "passed_runs": 0,
            "required_runs": DIFF_REQUIRED_RUNS,
            "min_cases_per_run": DIFF_MIN_CASES_PER_RUN,
            "last_status": "unknown",
        }

        # Autogenerated safety case path (text file)
        self.safety_case_rel: str = f"spec/reports/{self.name}_SafetyCase.md"

        # Persisted equivalence report (runner-written, model-read-only by policy)
        self.equiv_report_rel: str = f"spec/reports/{self.name}_EquivalenceReport.json"

    # ---------------- OpenAI helpers ----------------

    def _responses_create(
        self,
        *,
        instructions: str,
        input_data: Any,
        previous_response_id: Optional[str] = None,
        tool_choice: Optional[Any] = None,
        parallel_tool_calls: bool = False,
    ):
        kwargs: Dict[str, Any] = {
            "model": MODEL_ID,
            "instructions": instructions,
            "input": input_data,
            "tools": TOOLS_SCHEMA,
            "parallel_tool_calls": parallel_tool_calls,
        }
        if previous_response_id:
            kwargs["previous_response_id"] = previous_response_id
        if tool_choice is not None:
            kwargs["tool_choice"] = tool_choice
        return self.client.responses.create(**kwargs)

    def _tool_output_item(self, call_id: str, out: str) -> Dict[str, Any]:
        return {"type": "function_call_output", "call_id": call_id, "output": out}

    # ---------------- Equivalence report persistence ----------------

    def _load_equiv_report_if_present(self) -> None:
        p = Path(self.equiv_report_rel)
        if not p.exists():
            return
        try:
            rep = json.loads(_read_text_file(p))
            if isinstance(rep, dict) and rep.get("status") == "success":
                self._equiv_state["last_report"] = rep
                self._equiv_state["last_status"] = "success"
                self._equiv_state["passed_runs"] = int(rep.get("passed_runs", 0))
                self._equiv_state["required_runs"] = int(rep.get("required_runs", DIFF_REQUIRED_RUNS))
                self._equiv_state["min_cases_per_run"] = int(rep.get("min_cases_per_run", DIFF_MIN_CASES_PER_RUN))
        except Exception:
            # If malformed, ignore; we'll recompute during Equivalence.
            return

    def _persist_equiv_report_if_success(self) -> None:
        rep = self._equiv_state.get("last_report")
        if not isinstance(rep, dict):
            return
        if rep.get("status") != "success":
            return
        payload = {
            "status": "success",
            "saved_at_unix": int(time.time()),
            "passed_runs": rep.get("passed_runs"),
            "required_runs": rep.get("required_runs"),
            "min_cases_per_run": rep.get("min_cases_per_run"),
            "runs": rep.get("runs"),
            "total_time_s": rep.get("total_time_s"),
        }
        _write_text_file(Path(self.equiv_report_rel), json.dumps(payload, indent=2, sort_keys=True) + "\n")

    # ---------------- Tool execution with write lockdown + stage gating ----------------

    def _execute_tool_call(self, item) -> Tuple[Dict[str, Any], bool]:
        """
        Execute a single tool call from the model.
        Returns (function_call_output_item, success_flag_for_write/submit calls).
        """
        fname = item.name
        call_id = item.call_id
        try:
            args = json.loads(item.arguments) if item.arguments else {}
        except json.JSONDecodeError:
            args = {}

        # log args (hide content)
        log_args = dict(args)
        if "content" in log_args and isinstance(log_args["content"], str):
            log_args["content"] = f"<{len(log_args['content'])} chars>"
        log(f"Call: {fname}({json.dumps(log_args)})")

        # NOTE: RestartTranslationError must propagate to the outer loop.
        try:
            if fname == "restart_translation":
                reason = (args.get("reason") or "").strip()
                if not reason:
                    reason = "No reason provided."
                raise RestartTranslationError(reason)

            if fname == "read_source_file":
                rel = _safe_relpath(args["path"])
                p = self.source_root / rel
                if p.exists() and p.is_file():
                    out = _read_text_file(p)
                    if len(out) > MAX_TOOL_READ_CHARS:
                        out = out[:MAX_TOOL_READ_CHARS] + f"\n\n-- TRUNCATED at {MAX_TOOL_READ_CHARS} chars --"
                    return self._tool_output_item(call_id, out), True
                if p.exists() and p.is_dir():
                    return self._tool_output_item(call_id, f"Error: {rel} is a directory."), True
                return self._tool_output_item(call_id, f"Error: File not found {rel}"), True

            if fname == "read_lean_file":
                rel = _safe_relpath(args["path"]).replace("\\", "/")

                # Anti-smuggling: prevent reading Verif.lean before Equivalence has passed, unless
                # we're in stages where Verif is supposed to be authored.
                if rel == f"{self.name}/Verif.lean":
                    stage = self._current_stage.upper()
                    if stage not in ("SPECIFICATION", "HARDENING", "VERIFICATION"):
                        if self._equiv_state.get("last_status") != "success":
                            return self._tool_output_item(call_id, "Denied: Verif.lean is unavailable until after Equivalence succeeds."), False

                p = self.spec_src_root / rel
                if p.exists() and p.is_file():
                    out = _read_text_file(p)
                    if len(out) > MAX_TOOL_READ_CHARS:
                        out = out[:MAX_TOOL_READ_CHARS] + f"\n\n-- TRUNCATED at {MAX_TOOL_READ_CHARS} chars --"
                    return self._tool_output_item(call_id, out), True
                if p.exists() and p.is_dir():
                    return self._tool_output_item(call_id, f"Error: {rel} is a directory."), True
                return self._tool_output_item(call_id, f"Error: Lean file not found {rel}"), True

            if fname == "write_lean_file":
                rel = _safe_relpath(args["path"]).replace("\\", "/")

                # Stage-gated anti-smuggling: Verif.lean must not be edited until Equivalence succeeds.
                if rel == f"{self.name}/Verif.lean":
                    stage = self._current_stage.upper()
                    if stage not in ("SPECIFICATION", "HARDENING", "VERIFICATION"):
                        return self._tool_output_item(call_id, "Denied: Verif.lean is locked until Specification/Hardening."), False
                    if self._equiv_state.get("last_status") != "success":
                        return self._tool_output_item(call_id, "Denied: Equivalence has not succeeded; Verif.lean remains locked."), False

                # Enforce file-level lockdown
                if rel in self.locked_lean_paths:
                    msg = (
                        f"Denied: '{rel}' is locked (read-only). "
                        f"Locked: {sorted(self.locked_lean_paths)}"
                    )
                    return self._tool_output_item(call_id, msg), False

                # Enforce allowlist
                if rel not in self.allowed_lean_writes:
                    msg = (
                        f"Denied: '{rel}' is not in the autogenerated writable set. "
                        f"You may only write these Lean files:\n"
                        + "\n".join(sorted(self.allowed_lean_writes))
                    )
                    return self._tool_output_item(call_id, msg), False

                content = args.get("content", "")

                ok, reason = validate_basic_lean_shape(self.name, rel, content)
                if not ok:
                    return self._tool_output_item(call_id, f"Rejected content for '{rel}': {reason}"), False

                p = self.spec_src_root / rel
                _write_text_file(p, content)
                return self._tool_output_item(call_id, f"Written to {p}"), True

            if fname == "write_text_file":
                rel = _safe_relpath(args["path"]).replace("\\", "/")

                # Enforce allowlist
                if rel not in self.allowed_text_writes:
                    msg = (
                        f"Denied: '{rel}' is not in the autogenerated writable set. "
                        f"You may only write these files:\n"
                        + "\n".join(sorted(self.allowed_text_writes))
                    )
                    return self._tool_output_item(call_id, msg), False

                p = Path(rel)
                content = args.get("content", "")
                if not content.strip():
                    return self._tool_output_item(call_id, f"Rejected: '{rel}' content is empty."), False

                _write_text_file(p, content)
                return self._tool_output_item(call_id, f"Written to {p}"), True

            if fname == "verify_build":
                out = run_lake_build(self.spec_pkg_root)
                return self._tool_output_item(call_id, out), True

            if fname == "run_differential_test":
                out_json = self._run_differential_test_impl(args)
                log(f"[DiffTest Result] {out_json}")
                self._update_test_state_from_report(out_json)
                self._persist_equiv_report_if_success()
                return self._tool_output_item(call_id, out_json), True

            if fname == "submit_stage":
                ok, why = self._can_submit_current_stage()
                if not ok:
                    return self._tool_output_item(call_id, f"Denied submit_stage: {why}"), False
                out = f"Stage Submitted: {args.get('summary','')}"
                log(out)
                return self._tool_output_item(call_id, out), True

            return self._tool_output_item(call_id, f"Error: Unknown tool {fname}"), True

        except RestartTranslationError:
            raise
        except Exception as e:
            return self._tool_output_item(call_id, f"Tool execution error for {fname}: {e}"), False

    def _can_submit_current_stage(self) -> Tuple[bool, str]:
        """
        Hard gate: the model cannot "move on" by calling submit_stage unless
        the stage-specific success criteria are satisfied.
        """
        stage = self._current_stage.upper()

        # Always require the Lean project to build before leaving serious stages
        if stage in ("EQUIVALENCE", "HARDENING", "SPECIFICATION"):
            b = run_lake_build(self.spec_pkg_root)
            if not b.startswith("Build Success"):
                return False, "lake build is not successful; fix build errors first."
            # Also require the test harness to compile (not part of default build)
            if stage in ("EQUIVALENCE", "HARDENING"):
                hb = run_lake_build_target(self.spec_pkg_root, target="Spec.tests.Harness")
                if not hb.startswith("Build Success"):
                    return False, "harness build failed; fix Spec.tests.Harness compilation errors."

        if stage == "EQUIVALENCE":
            rep = self._equiv_state.get("last_report")
            if not rep:
                return False, "no differential test report yet; you must call run_differential_test."
            if self._equiv_state.get("last_status") != "success":
                return False, "differential testing has not succeeded; fix timeouts/diffs and rerun."
            if self._equiv_state.get("passed_runs", 0) < self._equiv_state.get("required_runs", DIFF_REQUIRED_RUNS):
                return False, f"insufficient successful runs; need {DIFF_REQUIRED_RUNS} passing seeds."
            return True, ""

        if stage == "HARDENING":
            if not Path(self.safety_case_rel).exists():
                return False, "safety case markdown file not written yet; write it to spec/reports/<project>_SafetyCase.md."
            if self._equiv_state.get("last_status") != "success":
                return False, "differential testing not successful after hardening; rerun and fix."
            if self._equiv_state.get("passed_runs", 0) < DIFF_REQUIRED_RUNS:
                return False, f"need {DIFF_REQUIRED_RUNS} passing seeds after hardening."
            return True, ""

        return True, ""

    # ---------------- Differential testing implementation (robust) ----------------

    def _update_test_state_from_report(self, report_json: str) -> None:
        try:
            rep = json.loads(report_json)
        except Exception:
            self._equiv_state["last_report"] = report_json
            self._equiv_state["last_status"] = "malformed_report"
            self._equiv_state["passed_runs"] = 0
            return

        self._equiv_state["last_report"] = rep
        self._equiv_state["last_status"] = rep.get("status", "unknown")
        self._equiv_state["passed_runs"] = rep.get("passed_runs", 0)
        self._equiv_state["required_runs"] = rep.get("required_runs", DIFF_REQUIRED_RUNS)
        self._equiv_state["min_cases_per_run"] = rep.get("min_cases_per_run", DIFF_MIN_CASES_PER_RUN)

    def _normalize_lean_harness_relpath(self, p: str) -> str:
        p = (p or "").replace("\\", "/").strip()
        # Common junk prefixes
        if p.startswith("./"):
            p = p[2:]
        if p.startswith("spec/Spec/"):
            p = p[len("spec/Spec/"):]
        if p.startswith("Spec/"):
            p = p[len("Spec/"):]
        return _safe_relpath(p)

    def _run_differential_test_impl(self, args: Dict[str, Any]) -> str:
        """
        Robust differential test runner:
        - Requires gen_inputs.py supports: --seed INT --n INT
        - Runs DIFF_REQUIRED_RUNS seeds
        - Requires >= DIFF_MIN_CASES_PER_RUN command lines per run
        - Compares stdout exactly between C and Lean
        Returns JSON string with status + details.
        """
        gen_script = _safe_relpath(args.get("gen_script_path", "spec/tests/gen_inputs.py"))
        c_harness = _safe_relpath(args.get("c_harness_path", "spec/tests/harness.c"))
        source_harness = c_harness  # generalize variable name
        raw = args.get("lean_harness_path", "tests/Harness.lean")
        lean_harness = self._normalize_lean_harness_relpath(raw)

        t0 = time.time()

        candidates = []
        # 1) user-provided (normalized)
        candidates.append(self.spec_src_root / lean_harness)
        # 2) canonical location (always)
        candidates.append(self.spec_src_root / "tests/Harness.lean")

        lean_run_path = None
        for cand in candidates:
            if cand.exists():
                lean_run_path = cand
                break

        if lean_run_path is None:
            # helpful debug: what did we try, and what's in spec/Spec/tests?
            tests_dir = self.spec_src_root / "tests"
            listing = []
            if tests_dir.exists():
                listing = sorted([x.name for x in tests_dir.iterdir() if x.is_file()])[:50]

            return json.dumps({
                "status": "error",
                "where": "lean_harness",
                "message": "Lean harness not found",
                "raw_arg": raw,
                "normalized": lean_harness,
                "tried": [str(c) for c in candidates],
                "tests_dir_listing": listing,
            })

        def _rc_desc(rc: int) -> str:
            if rc < 0:
                sig = -rc
                try:
                    return f"signal {signal.Signals(sig).name} ({rc})"
                except Exception:
                    return f"signal {sig} ({rc})"
            return str(rc)

        # 1) Compile Source Harness (Generic Support)
        # Build as objects so we can: (a) apply strict warnings to harness only,
        # (b) optionally silence project printf/puts without silencing harness,
        # (c) report exact sources/commands.
        exe_source = SPEC_TESTS_DIR / "harness.exe"
        build_dir = SPEC_TESTS_DIR / "build"
        build_dir.mkdir(parents=True, exist_ok=True)

        if str(source_harness).endswith(".c"):
            # Discover project .c files (exclude main.c)
            proj_c_srcs = [
                (self.source_root / f)
                for f in list_project_files(self.source_root)
                if f.endswith(".c") and "main.c" not in f.replace("\\", "/").lower()
            ]

            # Include dirs: root + any header dirs (helps “#include "engine.h"” style)
            include_dirs = {str(self.source_root.resolve())}
            for f in list_project_files(self.source_root):
                if f.endswith((".h", ".hpp")):
                    include_dirs.add(str((self.source_root / Path(f).parent).resolve()))
            include_flags = [flag for d in sorted(include_dirs) for flag in ["-I", d]]

            # Common flags
            COMMON = ["-std=c11", "-O2", "-g", "-fno-omit-frame-pointer"]

            # Make harness compilation strict about implicit decls (classic segfault cause)
            HARNESS_C = str(Path(source_harness))
            harness_o = build_dir / "harness.o"
            harness_cmd = [
                "gcc", *COMMON,
                "-Wall", "-Wextra",
                "-Werror=implicit-function-declaration",
                "-Werror=return-type",
                *include_flags,
                "-c", HARNESS_C, "-o", str(harness_o),
            ]

            # NOTE: We do NOT silence project stdout. The project's printf/puts output
            # is part of its observable semantics that the Lean translation must match.
            # If the project has debug noise, the model must handle it (replicate or filter).
            proj_macros: List[str] = []

            proj_os: List[Path] = []
            proj_compile_cmds: List[List[str]] = []
            for src in proj_c_srcs:
                obj = build_dir / (src.stem + ".o")
                proj_os.append(obj)
                proj_compile_cmds.append([
                    "gcc", *COMMON,
                    *proj_macros,
                    *include_flags,
                    "-c", str(src), "-o", str(obj),
                ])

            link_cmd = ["gcc", "-o", str(exe_source), str(harness_o)] + [str(o) for o in proj_os]

            # Run harness compile
            proc_h = subprocess.run(harness_cmd, capture_output=True, text=True)
            if proc_h.returncode != 0:
                return json.dumps({
                    "status": "error",
                    "where": "source_compile",
                    "message": "Harness compile failed",
                    "cmd": " ".join(shlex.quote(x) for x in harness_cmd),
                    "stderr": trunc(proc_h.stderr, 3000),
                    "stdout": trunc(proc_h.stdout, 1500),
                })

            # Compile project objects
            for cmd in proj_compile_cmds:
                proc_s = subprocess.run(cmd, capture_output=True, text=True)
                if proc_s.returncode != 0:
                    return json.dumps({
                        "status": "error",
                        "where": "source_compile",
                        "message": "Project source compile failed",
                        "cmd": " ".join(shlex.quote(x) for x in cmd),
                        "stderr": trunc(proc_s.stderr, 3000),
                        "stdout": trunc(proc_s.stdout, 1500),
                    })

            # Link
            proc_l = subprocess.run(link_cmd, capture_output=True, text=True)
            if proc_l.returncode != 0:
                return json.dumps({
                    "status": "error",
                    "where": "source_link",
                    "message": "Link failed",
                    "cmd": " ".join(shlex.quote(x) for x in link_cmd),
                    "stderr": trunc(proc_l.stderr, 3000),
                    "stdout": trunc(proc_l.stdout, 1500),
                })

        else:
            pass

        # 2) Ensure Lean builds
        log("[DiffTest] Starting lake build...")
        build_start = time.time()
        b_out = run_lake_build(self.spec_pkg_root)
        log(f"[DiffTest] lake build completed in {time.time() - build_start:.1f}s")
        if not b_out.startswith("Build Success"):
            return json.dumps({"status": "error", "where": "lean_build", "message": "Lean build failed", "build": trunc(b_out, 4000)})

        # Ensure the harness itself typechecks (it is not in the default build graph)
        log("[DiffTest] Building harness target Spec.tests.Harness...")
        harness_build_start = time.time()
        hb = run_lake_build_target(self.spec_pkg_root, target="Spec.tests.Harness")
        log(f"[DiffTest] Harness build completed in {time.time() - harness_build_start:.1f}s")
        if not hb.startswith("Build Success"):
            return json.dumps({
                "status": "error",
                "where": "lean_harness_build",
                "message": "Lean harness does not typecheck",
                "build": trunc_tail(hb, 4000),  # Use trunc_tail to show errors at end, not warnings at start
            })

        # 3) Run multiple seeds
        runs: List[Dict[str, Any]] = []
        passed = 0

        for k in range(DIFF_REQUIRED_RUNS):
            seed = DIFF_SEED_START + k

            try:
                gen_proc = subprocess.run(
                    [sys.executable, gen_script, "--seed", str(seed), "--n", str(DIFF_MIN_CASES_PER_RUN)],
                    capture_output=True,
                    text=True,
                    timeout=GEN_TIMEOUT_S,
                    check=False,
                )
            except subprocess.TimeoutExpired:
                return json.dumps({"status": "timeout", "where": "generator", "seed": seed, "message": f"gen_inputs.py exceeded {GEN_TIMEOUT_S}s"})
            except Exception as e:
                return json.dumps({"status": "error", "where": "generator", "seed": seed, "message": str(e)})

            if gen_proc.returncode != 0:
                return json.dumps({
                    "status": "error",
                    "where": "generator",
                    "seed": seed,
                    "message": "Generator must accept '--seed' and '--n' and exit 0.",
                    "stderr": trunc(gen_proc.stderr, 2000),
                    "stdout": trunc(gen_proc.stdout, 1200),
                })

            inputs = gen_proc.stdout
            lines = [ln for ln in inputs.splitlines() if ln.strip() != ""]
            num_cases = len(lines)

            if num_cases < DIFF_MIN_CASES_PER_RUN:
                return json.dumps({
                    "status": "insufficient_tests",
                    "where": "generator",
                    "seed": seed,
                    "message": f"Generator produced only {num_cases} cases; require >= {DIFF_MIN_CASES_PER_RUN}.",
                })

            # Run C
            log(f"[DiffTest] Running C harness (seed={seed}, {num_cases} cases)...")
            c_start = time.time()
            try:
                c_run = subprocess.run(
                    [str(exe_source)],
                    input=inputs,
                    capture_output=True,
                    text=True,
                    timeout=C_RUN_TIMEOUT_S
                )
                c_out = c_run.stdout
                c_err = c_run.stderr
                c_rc = c_run.returncode
                log(f"[DiffTest] C harness done in {time.time() - c_start:.2f}s (rc={c_rc})")
            except subprocess.TimeoutExpired:
                return json.dumps({
                    "status": "timeout",
                    "where": "c_run",
                    "seed": seed,
                    "message": f"C harness exceeded {C_RUN_TIMEOUT_S}s",
                    "inputs_tail": "\n".join(lines[-20:]),
                })
            except Exception as e:
                return json.dumps({
                    "status": "error",
                    "where": "c_run",
                    "seed": seed,
                    "message": str(e),
                    "inputs_tail": "\n".join(lines[-20:]),
                })

            if c_rc != 0:
                return json.dumps({
                    "status": "error",
                    "where": "c_run",
                    "seed": seed,
                    "message": f"C harness exited {_rc_desc(c_rc)}",
                    "inputs_tail": "\n".join(lines[-40:]),
                    "stderr": trunc(c_err, 3000),
                    "stdout": trunc(c_out, 3000),
                })

            # Run Lean
            log(f"[DiffTest] Running Lean harness (seed={seed})...")
            lean_start = time.time()
            try:
                lean_cmd = ["lake", "env", "lean", "--run", str(lean_run_path)]
                lean_run = subprocess.run(
                    lean_cmd,
                    cwd=str(self.spec_pkg_root),
                    input=inputs,
                    capture_output=True,
                    text=True,
                    timeout=LEAN_RUN_TIMEOUT_S,
                )
                lean_out = lean_run.stdout
                lean_err = lean_run.stderr
                lean_rc = lean_run.returncode
                log(f"[DiffTest] Lean harness done in {time.time() - lean_start:.2f}s (rc={lean_rc})")
            except subprocess.TimeoutExpired:
                return json.dumps({
                    "status": "timeout",
                    "where": "lean_run",
                    "seed": seed,
                    "message": f"Lean harness exceeded {LEAN_RUN_TIMEOUT_S}s; optimize harness/parsing and avoid slow per-line IO.",
                })
            except Exception as e:
                return json.dumps({"status": "error", "where": "lean_run", "seed": seed, "message": str(e)})

            if lean_rc != 0:
                return json.dumps({
                    "status": "error",
                    "where": "lean_run",
                    "seed": seed,
                    "message": f"Lean harness exited {lean_rc}",
                    "stderr": trunc(lean_err, 2500),
                    "stdout": trunc(lean_out, 2500),
                })

            if c_out == lean_out:
                passed += 1
                runs.append({"seed": seed, "cases": num_cases, "status": "pass"})
                continue

            return json.dumps({
                "status": "diff",
                "where": "compare",
                "seed": seed,
                "cases": num_cases,
                "message": "Outputs differ; fix Lean semantics or harness to match C.",
                "c_out": trunc(c_out, 2500),
                "lean_out": trunc(lean_out, 2500),
            })

        total_s = time.time() - t0
        return json.dumps({
            "status": "success",
            "passed_runs": passed,
            "required_runs": DIFF_REQUIRED_RUNS,
            "min_cases_per_run": DIFF_MIN_CASES_PER_RUN,
            "runs": runs,
            "total_time_s": round(total_s, 3),
        })

    # ---------------- Structure generation and lockdown ----------------

    def _register_module_in_package_root(self) -> None:
        """
        Ensure spec/Spec.lean imports Spec.<project>.
        This file is outside spec/Spec/, so the model cannot write it anyway.
        """
        spec_file = self.spec_src_root.parent / "Spec.lean"  # spec/Spec.lean
        line = f"import Spec.{self.name}"
        if spec_file.exists():
            content = _read_text_file(spec_file)
            if line not in content:
                _write_text_file(spec_file, content.rstrip() + "\n" + line + "\n")
        else:
            _write_text_file(spec_file, line + "\n")

    def _write_project_root_module_locked(self, project_module_paths: List[str]) -> None:
        """
        Write spec/Spec/<project>.lean importing all project submodules.
        The model is forbidden from writing this file.
        """
        imports: List[str] = []
        for rel in project_module_paths:
            p = Path(rel)
            if p.suffix != ".lean":
                continue
            mod = "Spec." + ".".join(p.with_suffix("").parts)
            imports.append(f"import {mod}")

        # Always import Verif.lean so it is included in the project build (Aristotle requires a single project).
        imports.append(f"import Spec.{self.name}.Verif")

        root_rel = f"{self.name}.lean"
        body = "\n".join(sorted(set(imports))) + "\n"
        _write_text_file(self.spec_src_root / root_rel, body)
        self.locked_lean_paths.add(root_rel)

    def _autogen_scaffold_and_lockdown(self) -> None:
        """
        Auto-generate the Lean file structure inside spec/Spec and lock down writes so the model
        can only write to those autogenerated files. Also autogenerate test harness paths and allow
        writing only to them (not arbitrary files).
        """
        ensure_prelude_and_lockdown()

        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        if not src_files:
            log(f"No C/C++ source files found under {self.source_root}")
            return

        used_names: Dict[str, int] = {}
        project_module_paths: List[str] = []

        self.src_to_lean.clear()
        self.lean_to_src.clear()

        for rel in src_files:
            out_rel = _lean_out_path_for_source(self.name, rel, used_names)
            self.src_to_lean[rel] = out_rel
            self.lean_to_src[out_rel] = rel
            project_module_paths.append(out_rel)

        # Autogenerate stub modules (model-writable)
        self.allowed_lean_writes = set(project_module_paths)

        for out_rel in project_module_paths:
            p = self.spec_src_root / out_rel
            if not p.exists():
                stub = (
                    "import Spec.Prelude\n\n"
                    f"namespace Spec.{self.name}\n\n"
                    f"-- AUTOGENERATED STUB\n"
                    f"-- Source: {self.lean_to_src.get(out_rel, '(unknown)')}\n"
                    f"-- SAFETY-CRITICAL MODE: you must implement full semantics; no placeholders/stubs.\n\n"
                    f"end Spec.{self.name}\n"
                )
                _write_text_file(p, stub)

        # Verif module (model-writable but stage-gated)
        verif_rel = f"{self.name}/Verif.lean"
        self.allowed_lean_writes.add(verif_rel)
        if not (self.spec_src_root / verif_rel).exists():
            verif_stub = (
                "import Spec.Prelude\n\n"
                f"namespace Spec.{self.name}\n\n"
                "-- AUTOGENERATED SPEC STUB (PROOFS-ONLY)\n"
                "-- This file is imported for Aristotle; to prevent smuggling it must not define executable entities.\n"
                "-- You may write theorems/lemmas here; 'sorry' is allowed here only.\n\n"
                f"end Spec.{self.name}\n"
            )
            _write_text_file(self.spec_src_root / verif_rel, verif_stub)

        # Lean harness (model-writable)
        harness_rel = "tests/Harness.lean"
        self.allowed_lean_writes.add(harness_rel)
        harness_path = self.spec_src_root / harness_rel
        if not harness_path.exists():
            harness_stub = (
                "import Spec.Prelude\n"
                f"import Spec.{self.name}\n\n"
                f"namespace Spec.{self.name}\n\n"
                "-- AUTOGENERATED HARNESS STUB\n"
                "-- Implement a fast stdin parser and deterministic stdout printer.\n"
                "-- Must be efficient: avoid slow per-line IO and quadratic string concatenation.\n\n"
                "def main : IO Unit := do\n"
                "  -- Implement command loop\n"
                "  pure ()\n\n"
                f"end Spec.{self.name}\n"
            )
            _write_text_file(harness_path, harness_stub)

        # Autogenerate non-Lean harness files in spec/tests (model-writable)
        SPEC_TESTS_DIR.mkdir(parents=True, exist_ok=True)
        SPEC_REPORTS_DIR.mkdir(parents=True, exist_ok=True)

        gen_rel = "spec/tests/gen_inputs.py"
        c_rel = "spec/tests/harness.c"
        self.allowed_text_writes = {gen_rel, c_rel, self.safety_case_rel}

        if not Path(gen_rel).exists():
            _write_text_file(Path(gen_rel),
                "#!/usr/bin/env python3\n"
                "import argparse\n"
                "import random\n"
                "\n"
                "# AUTOGENERATED GENERATOR STUB\n"
                "# Contract: must accept --seed INT and --n INT and print >= n non-empty command lines.\n"
                "# Output is fed verbatim to both harnesses.\n"
                "\n"
                "def main():\n"
                "    ap = argparse.ArgumentParser()\n"
                "    ap.add_argument('--seed', type=int, required=True)\n"
                "    ap.add_argument('--n', type=int, required=True)\n"
                "    args = ap.parse_args()\n"
                "    random.seed(args.seed)\n"
                "    for _ in range(args.n):\n"
                "        print('NOOP')\n"
                "\n"
                "if __name__ == '__main__':\n"
                "    main()\n"
            )

        if not Path(c_rel).exists():
            _write_text_file(Path(c_rel),
                "/* AUTOGENERATED C HARNESS STUB\n"
                " * Read commands from stdin, execute against the C implementation, and print deterministic stdout.\n"
                " * Must match Lean harness output exactly.\n"
                " */\n"
                "#include <stdio.h>\n"
                "#include <string.h>\n"
                "\n"
                "int main(void) {\n"
                "    char buf[512];\n"
                "    while (fgets(buf, sizeof(buf), stdin)) {\n"
                "        size_t n = strlen(buf);\n"
                "        while (n && (buf[n-1] == '\\n' || buf[n-1] == '\\r')) { buf[n-1] = 0; n--; }\n"
                "        if (n == 0) continue;\n"
                "        // TODO: parse and execute commands; print results\n"
                "        if (strcmp(buf, \"NOOP\") == 0) {\n"
                "            puts(\"OK\");\n"
                "        } else {\n"
                "            puts(\"ERR\");\n"
                "        }\n"
                "    }\n"
                "    return 0;\n"
                "}\n"
            )

        # Autogenerate safety case placeholder (model-writable)
        if not Path(self.safety_case_rel).exists():
            _write_text_file(Path(self.safety_case_rel),
                f"# Safety Case (AUTOGENERATED)\n\n"
                f"Project: `{self.name}`\n\n"
                f"> This file must be rewritten by the agent during the HARDENING stage.\n"
            )

        # Write project root module (locked)
        self._write_project_root_module_locked(project_module_paths)

        # Ensure package root imports project
        self._register_module_in_package_root()

        # Lock Prelude and project root module
        self.locked_lean_paths.add("Prelude.lean")
        self.locked_lean_paths.add(f"{self.name}.lean")

        # If an equivalence report was persisted, load it (supports SKIP_TO_HARDENING workflows safely).
        self._load_equiv_report_if_present()

        log("Autogenerated Spec/Spec scaffold and locked down writes.")
        log(f"Locked Lean files (read-only for model): {sorted(self.locked_lean_paths)}")
        log(f"Writable Lean files (model may edit): {len(self.allowed_lean_writes)} files")
        log(f"Writable text files (model may edit): {sorted(self.allowed_text_writes)}")

    # ---------------- Prompt context helpers ----------------

    def _spec_layout_for_prompt(self) -> str:
        shown: List[str] = []
        candidates = list_lean_files(self.spec_src_root)
        priority = []
        for p in candidates:
            if p == "Prelude.lean":
                priority.append(p)
            elif p == f"{self.name}.lean":
                priority.append(p)
            elif p.startswith(f"{self.name}/"):
                priority.append(p)
            elif p.startswith("tests/"):
                priority.append(p)
        seen = set()
        for p in priority:
            if p not in seen:
                shown.append(p)
                seen.add(p)
        shown = _limit_lines(shown, 240)
        return "\n".join(shown)

    def _source_layout_for_prompt(self) -> str:
        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        src_files = _limit_lines(src_files, 240)
        return "\n".join(src_files)

    def _mapping_for_prompt(self) -> str:
        lines: List[str] = []
        for src, out in sorted(self.src_to_lean.items()):
            lines.append(f"{src}  ->  {out}")
        lines = _limit_lines(lines, 280)
        return "\n".join(lines)

    def _writable_files_for_prompt(self) -> str:
        lines = sorted(self.allowed_lean_writes)
        lines = _limit_lines(lines, 280)
        return "\n".join(lines)

    def _locked_files_for_prompt(self) -> str:
        return "\n".join(sorted(self.locked_lean_paths))

    def _base_instructions(self, stage: str, focus_src: Optional[str] = None, focus_out: Optional[str] = None) -> str:
        focus_lines = ""
        if focus_src and focus_out:
            focus_lines = (
                f"FOCUS FILE (source -> target): {focus_src}  ->  {focus_out}\n"
                f"You must ultimately update ONLY: {focus_out}\n"
            )

        return (
            "ROLE: You are Anneal's translation engine: an expert engineer translating arbitrary input source code into Lean 4.\n"
            "SETTING: SAFETY-CRITICAL. You must assume correctness is mandatory. No placeholders, no stubs, no 'return true'.\n"
            "OBJECTIVE: produce Lean 4 code that compiles in the Lake project under spec/ and matches C semantics.\n\n"
            f"STAGE: {stage}\n"
            f"PROJECT: {self.name}\n"
            f"SOURCE ROOT: {self.source_root}\n"
            f"SPEC ROOT: {SPEC_DIR}\n"
            f"SPEC SOURCE ROOT: {SPEC_SRC_DIR}\n\n"
            f"{focus_lines}"
            "HARD CONSTRAINTS (enforced by runner):\n"
            "- You may ONLY write Lean files that were autogenerated by the runner.\n"
            "- Locked Lean files are read-only (do not attempt to modify them):\n"
            f"{self._locked_files_for_prompt()}\n\n"
            "- Import policy for generated modules:\n"
            "  * Each generated module MUST include 'import Spec.Prelude'.\n"
            "  * Do NOT import Std or Mathlib directly inside generated modules.\n"
            "  * You may import Spec.<project>.* only if necessary.\n\n"
            "CURRENT SPEC LAYOUT (key Lean files):\n"
            f"{self._spec_layout_for_prompt()}\n\n"
            "CURRENT SOURCE LAYOUT (C/C++ files):\n"
            f"{self._source_layout_for_prompt()}\n\n"
            "SOURCE -> LEAN MAPPING (autogenerated):\n"
            f"{self._mapping_for_prompt()}\n\n"
            "LEAN FILES YOU ARE ALLOWED TO WRITE:\n"
            f"{self._writable_files_for_prompt()}\n\n"
            "SAFETY-CRITICAL RULES:\n"
            "- Never insert placeholder behavior. If you cannot implement something, you must read more files and implement it.\n"
            "- Do not 'assume true', 'stub', 'simplify away' semantics, or explain that you'd do more later.\n"
            "- Translated/runtime modules MUST NOT use 'sorry'. Only Verif.lean may contain sorry.\n"
            "- Verif.lean is proofs-only (anti-smuggling): no defs/instances/axioms/macros.\n\n"
            "WORKING STYLE:\n"
            "- Prefer simple, dependable data structures first (List/Array/Option/Nat/Int) *as long as semantics match*.\n"
            "- Keep IO minimal and deterministic.\n"
            "- When fixing build errors: resolve the first error with the smallest correct change.\n"
        )

    def _project_summary(self) -> str:
        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        blobs: List[str] = []
        for rel in src_files[:8]:
            txt = _read_text_file(self.source_root / rel)
            blobs.append(f"FILE: {rel}\n" + trunc(txt, 2800) + "\n")
        instructions = (
            "Summarize this C/C++ codebase for a Lean translator.\n"
            "Return a concise, technical summary: key data structures, APIs, invariants, and control flow.\n"
            "Assume this is safety-critical production code.\n"
        )
        resp = self._responses_create(instructions=instructions, input_data="\n".join(blobs))
        summary = getattr(resp, "output_text", None) or "No summary."
        return summary

    # ---------------- Session driver: allow reads; require a successful write ----------------

    def _session_until_successful_write(
        self,
        *,
        stage: str,
        focus_src: str,
        focus_out: str,
        user_payload: str,
        max_turns: int = MAX_SESSION_TURNS,
    ) -> bool:
        instructions = self._base_instructions(stage=stage, focus_src=focus_src, focus_out=focus_out)
        previous_response_id: Optional[str] = None
        current_input: Any = user_payload

        for turn in range(max_turns):
            log(f"Turn {turn+1}")
            resp = self._responses_create(
                instructions=instructions,
                input_data=current_input,
                previous_response_id=previous_response_id,
                tool_choice=None,
                parallel_tool_calls=False,
            )
            previous_response_id = resp.id

            tool_calls = []
            if getattr(resp, "output", None):
                for item in resp.output:
                    if item.type == "message":
                        for part in item.content:
                            if part.type == "output_text":
                                log(f"Model: {trunc(part.text)}")
                    elif item.type == "function_call":
                        tool_calls.append(item)

            if not tool_calls:
                log("No tool calls; nudging model.")
                current_input = (
                    "NO TOOL CALLS DETECTED.\n"
                    "You MUST call tools to modify files or verifying builds.\n"
                    "If you are finished or stuck, explain why, but you must attempt a tool call first.\n"
                )
                continue

            tool_outputs: List[Dict[str, Any]] = []
            wrote_ok = False

            for call in tool_calls:
                out_item, ok = self._execute_tool_call(call)
                tool_outputs.append(out_item)

                if call.name == "write_lean_file":
                    try:
                        a = json.loads(call.arguments) if call.arguments else {}
                    except json.JSONDecodeError:
                        a = {}
                    path = (a.get("path", "") or "").replace("\\", "/")
                    if ok and path == focus_out:
                        wrote_ok = True

            current_input = tool_outputs

            if wrote_ok:
                return True

        log("Session exceeded max turns without a successful write to the focus file.")
        return False

    # ---------------- Translation + convergence ----------------

    def _translate_file(self, *, project_summary: str, src_rel: str, out_rel: str) -> bool:
        src_txt = _read_text_file(self.source_root / src_rel)
        existing = _read_text_file(self.spec_src_root / out_rel) if (self.spec_src_root / out_rel).exists() else ""

        user_payload = (
            "TASK: Translate the given source file into the target Lean file.\n"
            "You may call read_source_file / read_lean_file if needed.\n"
            "You MUST call write_lean_file to fully overwrite the target Lean file.\n\n"
            "PROJECT SUMMARY:\n"
            f"{project_summary}\n\n"
            f"SOURCE FILE: {src_rel}\n"
            "----- BEGIN SOURCE CONTENT -----\n"
            f"{src_txt}\n"
            "----- END SOURCE CONTENT -----\n\n"
            f"TARGET LEAN FILE: {out_rel}\n"
            "----- BEGIN CURRENT TARGET CONTENT -----\n"
            f"{trunc(existing, 6500)}\n"
            "----- END CURRENT TARGET CONTENT -----\n\n"
            "REQUIREMENTS:\n"
            f"- The Lean file MUST be in namespace Spec.{self.name}\n"
            "- It MUST import Spec.Prelude\n"
            "- Implement real semantics; no placeholders or constant returns for integrity checks.\n"
            "- Keep deterministic behavior.\n"
        )

        ok = self._session_until_successful_write(
            stage="TRANSLATION",
            focus_src=src_rel,
            focus_out=out_rel,
            user_payload=user_payload,
            max_turns=MAX_SESSION_TURNS,
        )
        return ok

    def _repair_file_until_builds(self, *, project_summary: str, src_rel: str, out_rel: str) -> bool:
        target_mod = module_name_from_lean_path(out_rel)
        if not target_mod:
            log(f"Cannot compute module name for {out_rel}")
            return False

        out = run_lake_build_target(self.spec_pkg_root, target=target_mod)
        if out.startswith("Build Success"):
            return True

        for step in range(MAX_REPAIR_TURNS_PER_FILE):
            errs = parse_lean_errors(out, max_n=6)
            p = self.spec_src_root / out_rel
            cur = _read_text_file(p) if p.exists() else ""

            if errs:
                e0 = errs[0]
                snippet = excerpt_around(cur, e0.line, radius=16)
                err_lines = "\n".join([f"{e.file}:{e.line}:{e.col}: {e.msg}" for e in errs])
            else:
                snippet = trunc(cur, 1400)
                err_lines = trunc(out, 2200)

            src_txt = _read_text_file(self.source_root / src_rel)

            user_payload = (
                "TASK: Fix the target Lean file so that it compiles, WITHOUT changing intended semantics.\n"
                "You MUST edit ONLY the target Lean file.\n"
                "You may read other files if necessary, but the final action must be write_lean_file on the target.\n\n"
                "PROJECT SUMMARY:\n"
                f"{project_summary}\n\n"
                f"SOURCE FILE (truth): {src_rel}\n"
                "----- BEGIN SOURCE CONTENT -----\n"
                f"{trunc(src_txt, 7500)}\n"
                "----- END SOURCE CONTENT -----\n\n"
                f"TARGET LEAN FILE: {out_rel}\n"
                "----- BEGIN CURRENT TARGET CONTENT (truncated) -----\n"
                f"{trunc(cur, 7500)}\n"
                "----- END CURRENT TARGET CONTENT -----\n\n"
                "BUILD ERRORS (focus on the FIRST one):\n"
                f"{trunc(err_lines, 2600)}\n\n"
                "SNIPPET AROUND FIRST ERROR:\n"
                f"{snippet}\n\n"
                "REPAIR RULES:\n"
                "- Fix the FIRST error with the smallest correct change.\n"
                "- Keep import Spec.Prelude only.\n"
                f"- Keep namespace Spec.{self.name}.\n"
                "- Do not introduce placeholders/stubs.\n"
            )

            ok = self._session_until_successful_write(
                stage=f"REPAIR (file) step {step+1}",
                focus_src=src_rel,
                focus_out=out_rel,
                user_payload=user_payload,
                max_turns=MAX_SESSION_TURNS,
            )
            if not ok:
                log("Repair session stalled.")
                return False

            out = run_lake_build_target(self.spec_pkg_root, target=target_mod)
            log(trunc(out, 1800))
            if out.startswith("Build Success"):
                return True

        return False

    def _repair_project_until_builds(self, *, project_summary: str) -> None:
        """
        Global repair: keep fixing the file referenced by the first error until the full project builds.

        IMPORTANT:
        - If the first error is in a LOCKED root module (e.g. Spec/<project>.lean) due to an import failure,
          we "unwrap" it and repair the imported module instead.
        - We only regenerate the root module if it is genuinely stale; otherwise we avoid infinite loops.
        """
        # guard to prevent infinite "regen root module" loops
        regen_root_streak = 0

        for step in range(MAX_REPAIR_TURNS_GLOBAL):
            out = run_lake_build(self.spec_pkg_root)
            if out.startswith("Build Success"):
                return

            errs = parse_lean_errors(out, max_n=6)
            if not errs:
                log("Global repair: cannot parse Lean errors; stopping.")
                raise RuntimeError("Cannot parse build errors for global repair.")

            e0 = errs[0]

            # Normalize Lean error "file" to spec/Spec relative when possible.
            file_rel = e0.file.replace("\\", "/")
            if file_rel.startswith("Spec/"):
                file_rel = file_rel[len("Spec/"):]  # drop "Spec/"
            # file_rel looks like: "order_engine.lean" or "order_engine/Engine2.lean"

            # If the first error is in a locked root module, try to unwrap the import failure.
            if file_rel in self.locked_lean_paths:
                # Prelude repairs stay as-is
                if file_rel == "Prelude.lean":
                    ensure_prelude_and_lockdown()
                    log("Global repair: Prelude re-ensured.")
                    regen_root_streak = 0
                    continue

                # Project root module: unwrap import failures instead of infinite regen
                if file_rel == f"{self.name}.lean":
                    failed_mod = extract_failed_import_module(e0.msg)

                    # If we can identify the failing import, target that module's file directly.
                    if failed_mod:
                        target_rel = module_to_spec_relpath(failed_mod)
                        if target_rel and target_rel in self.allowed_lean_writes and target_rel not in self.locked_lean_paths:
                            log(f"Global repair: root import failed; targeting {target_rel} (from {file_rel}).")
                            file_rel = target_rel  # fallthrough into normal writable repair below
                        else:
                            # If it's not writable, regen root once (staleness) then hard-stop with context.
                            regen_root_streak += 1
                            if regen_root_streak <= 1:
                                project_module_paths = sorted([
                                    p for p in self.allowed_lean_writes
                                    if p.startswith(f"{self.name}/") and p.endswith(".lean")
                                ])
                                self._write_project_root_module_locked(project_module_paths)
                                log("Global repair: regenerated project root module (staleness / non-writable import target).")
                                continue
                            raise RuntimeError(
                                f"Build failed in locked root module {file_rel} due to import {failed_mod}, "
                                "but the target is not writable. Cannot proceed safely.\n"
                                + trunc(out, 4000)
                            )
                    else:
                        # No import failure info. Regenerate root once, then stop to avoid looping.
                        regen_root_streak += 1
                        if regen_root_streak <= 1:
                            project_module_paths = sorted([
                                p for p in self.allowed_lean_writes
                                if p.startswith(f"{self.name}/") and p.endswith(".lean")
                            ])
                            self._write_project_root_module_locked(project_module_paths)
                            log("Global repair: regenerated project root module (no import failure parsed).")
                            continue
                        raise RuntimeError(
                            f"Build failed in locked root module {file_rel}, but could not unwrap a failing import.\n"
                            + trunc(out, 4000)
                        )

                # If file_rel is still locked (i.e. we didn't successfully unwrap/re-target), then fail.
                if file_rel in self.locked_lean_paths:
                    raise RuntimeError(f"Build failed in locked file {file_rel}; cannot proceed safely.\n{trunc(out, 4000)}")

            # If we got here, we're repairing a writable Lean file.
            regen_root_streak = 0

            if file_rel not in self.allowed_lean_writes:
                raise RuntimeError(f"Build failed in non-writable file {file_rel}; cannot proceed safely.\n{trunc(out, 4000)}")

            src_rel = self.lean_to_src.get(file_rel, "(unknown source)")
            cur = _read_text_file(self.spec_src_root / file_rel)

            snippet = excerpt_around(cur, e0.line, radius=16)
            err_lines = "\n".join([f"{e.file}:{e.line}:{e.col}: {e.msg}" for e in errs])

            user_payload = (
                "TASK: Fix the referenced Lean file so the project builds.\n"
                "This is a global convergence loop. You must implement real semantics; no placeholders.\n\n"
                "PROJECT SUMMARY:\n"
                f"{project_summary}\n\n"
                f"FAILING FILE: {file_rel}\n"
                f"SOURCE (if known): {src_rel}\n\n"
                "CURRENT CONTENT (truncated):\n"
                f"{trunc(cur, 7500)}\n\n"
                "BUILD ERRORS:\n"
                f"{trunc(err_lines, 2800)}\n\n"
                "SNIPPET:\n"
                f"{snippet}\n"
            )

            ok = self._session_until_successful_write(
                stage=f"REPAIR (global) step {step+1}",
                focus_src=src_rel,
                focus_out=file_rel,
                user_payload=user_payload,
                max_turns=MAX_SESSION_TURNS,
            )
            if not ok:
                raise RuntimeError("Global repair session stalled.")

        raise RuntimeError("Global repair exceeded max steps; refusing to proceed.")

    def run_stage_translation(self, restart_reason: Optional[str] = None) -> None:
        log("--- Stage: Translation ---")
        self._current_stage = "TRANSLATION"

        if restart_reason:
            log(f"[RESTART] Previous translation failed due to: {restart_reason[:200]}...")

        project_summary = self._project_summary()
        if restart_reason:
            # Inject the restart reason into the project summary so model sees it
            project_summary = (
                f"!!! IMPORTANT: A previous translation attempt failed during differential testing !!!\n"
                f"Failure reason: {restart_reason}\n\n"
                f"Pay close attention to faithfully matching the C semantics, especially:\n"
                f"- Free-list ordering (LIFO stack behavior)\n"
                f"- Pointer/index identity across operations\n"
                f"- Numeric parsing (strtoul-like behavior)\n"
                f"- Edge cases in allocator exhaustion\n\n"
                + project_summary
            )
        log("Project summary computed.")

        # Translate each source file to its target, then repair until that module builds.
        # After each file converges locally, do a full `lake build` and do not move on until the full project builds.
        for src_rel, out_rel in sorted(self.src_to_lean.items()):
            log(f"Translating {src_rel} -> {out_rel}")

            ok = self._translate_file(project_summary=project_summary, src_rel=src_rel, out_rel=out_rel)
            if not ok:
                raise RuntimeError(f"Translation session failed for {src_rel}")

            ok = self._repair_file_until_builds(project_summary=project_summary, src_rel=src_rel, out_rel=out_rel)
            if not ok:
                raise RuntimeError(f"Failed to converge {out_rel} to a compiling state")

            # Full project build check after each file
            log("Full-project build check (after per-file convergence)...")
            self._repair_project_until_builds(project_summary=project_summary)
            log("Full-project build OK. Proceeding to next file.")

        # Final full build (sanity check).
        out = run_lake_build(self.spec_pkg_root)
        if not out.startswith("Build Success"):
            raise RuntimeError("Translation complete but lake build still fails (should not happen).")

        log("Translation complete: lake build succeeded.")

    # ---------------- Equivalence (robust) ----------------

    def run_stage_equivalence(self) -> None:
        log("--- Stage: Equivalence ---")
        self._current_stage = "EQUIVALENCE"

        project_files = sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/")])
        instructions = self._base_instructions(stage="EQUIVALENCE")

        brief = (
            "TASK: Implement ROBUST differential testing in a safety-critical setting.\n"
            "You must implement these writable files:\n"
            f"- {self.safety_case_rel} (NOT in this stage; that's hardening)\n"
            "- spec/tests/gen_inputs.py\n"
            "- spec/tests/harness.c\n"
            "- tests/Harness.lean (under spec/Spec/)\n\n"
            "Generator contract (MANDATORY):\n"
            "- gen_inputs.py must accept: --seed INT --n INT\n"
            "- It must print >= n non-empty command lines.\n"
            "- It must generate diverse and adversarial sequences of operations (random + edge cases).\n\n"
            "Harness contract (MANDATORY):\n"
            "- Both harnesses must parse the exact same command language.\n"
            "- Both must print deterministic stdout for each command.\n"
            "- Keep it fast; avoid slow per-line Lean IO patterns.\n"
            "- C harness: use standard C11. For strtok_r, add '#define _POSIX_C_SOURCE 200809L' before <string.h>.\n"
            "- C harness: do NOT #include .c files directly. Only #include headers; linker provides implementations.\n"
            "- Lean harness: The 'main' function MUST be visible to 'lean --run'. Either:\n"
            "  (a) define 'def main' OUTSIDE any namespace, or\n"
            "  (b) annotate with '@[main] def main' inside a namespace.\n"
            "  If you get 'unknown declaration main', you probably have it inside a namespace without @[main].\n\n"
            "Testing requirement (MANDATORY):\n"
            f"- You MUST call run_differential_test until it returns JSON status=success with passed_runs={DIFF_REQUIRED_RUNS}.\n"
            f"- Each run must have >= {DIFF_MIN_CASES_PER_RUN} cases.\n"
            "- If you get timeouts or diffs, fix the problem and rerun.\n"
            "- If the C harness crashes, treat it as a bug in generator/harness input validation or in the source engine; do NOT call restart_translation unless you have a confirmed semantic mismatch (status=diff).\n"
            "- You may edit translated Lean files to correct semantic mismatches.\n\n"
            "Note: you cannot finish this stage by calling submit_stage unless tests pass.\n\n"
            "Translated project Lean files:\n"
            + "\n".join(_limit_lines(project_files, 120))
        )

        previous_response_id: Optional[str] = None
        current_input: Any = brief

        for turn in range(60):
            log(f"Turn {turn+1}")
            resp = self._responses_create(
                instructions=instructions,
                input_data=current_input,
                previous_response_id=previous_response_id,
                tool_choice=None,
                parallel_tool_calls=False,
            )
            previous_response_id = resp.id

            tool_calls = []
            if getattr(resp, "output", None):
                for item in resp.output:
                    if item.type == "message":
                        for part in item.content:
                            if part.type == "output_text":
                                log(f"Model: {trunc(part.text)}")
                    elif item.type == "function_call":
                        tool_calls.append(item)

            if not tool_calls:
                # Nudge the model back into tool-using mode instead of crashing.
                current_input = (
                    "NO TOOL CALLS DETECTED.\n"
                    "You MUST call at least one tool each turn.\n"
                    "Next action: call run_differential_test with:\n"
                    "- gen_script_path = spec/tests/gen_inputs.py\n"
                    "- c_harness_path = spec/tests/harness.c\n"
                    "- lean_harness_path = tests/Harness.lean\n"
                    "If it fails, read the failing file(s), fix, and retry.\n"
                )
                continue

            tool_outputs: List[Dict[str, Any]] = []
            submit_ok = False

            for call in tool_calls:
                out_item, ok = self._execute_tool_call(call)
                tool_outputs.append(out_item)
                if call.name == "submit_stage" and ok:
                    submit_ok = True

            current_input = tool_outputs

            if submit_ok:
                break

        out = run_lake_build(self.spec_pkg_root)
        if not out.startswith("Build Success"):
            raise RuntimeError("Equivalence ended but build fails.")

        if self._equiv_state.get("last_status") != "success" or self._equiv_state.get("passed_runs", 0) < DIFF_REQUIRED_RUNS:
            raise RuntimeError("Equivalence ended without robust passing differential tests (should not happen due to gating).")

        log("Equivalence complete: robust differential tests passed.")

    # ---------------- Specification (stronger prompt) ----------------

    def run_stage_specification(self) -> None:
        log("--- Stage: Specification ---")
        self._current_stage = "SPECIFICATION"

        out_rel = f"{self.name}/Verif.lean"
        src_list = "\n".join(_limit_lines(sorted(self.src_to_lean.keys()), 120))
        lean_list = "\n".join(_limit_lines(sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/")]), 160))

        sample_files = _limit_lines(sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/") and p.endswith(".lean")]), 6)
        sample_blob: List[str] = []
        for rel in sample_files:
            p = self.spec_src_root / rel
            if p.exists():
                sample_blob.append(f"FILE: {rel}\n" + trunc(_read_text_file(p), 2800) + "\n")

        user_payload = (
            "TASK: Write a SPECIFICATION for the translated project.\n"
            "This is SAFETY-CRITICAL: the spec must be meaningful, not generic.\n"
            "Proofs may use sorry here, but statements must connect to the translated definitions.\n"
            "IMPORTANT: Verif.lean is proofs-only (anti-smuggling): do NOT introduce def/abbrev/instance/axiom/etc.\n\n"
            f"TARGET: {out_rel}\n\n"
            "REQUIREMENTS:\n"
            "- Include at least 10 non-trivial theorems/lemmas.\n"
            "- Include invariants about memory safety / bounds (arrays/pools), functional correctness conditions, and determinism.\n"
            "- Include at least 2 theorems that connect a multi-step scenario to postconditions.\n"
            "- Make sure the file parses and typechecks.\n\n"
            "SOURCE FILES:\n"
            f"{src_list}\n\n"
            "TRANSLATED LEAN FILES:\n"
            f"{lean_list}\n\n"
            "SAMPLE LEAN CONTENT:\n"
            + "\n\n".join(sample_blob)
        )

        ok = self._session_until_successful_write(
            stage="SPECIFICATION",
            focus_src="(project)",
            focus_out=out_rel,
            user_payload=user_payload,
            max_turns=MAX_SESSION_TURNS,
        )
        if not ok:
            raise RuntimeError("Specification session stalled.")

        # Repair loop: if Verif.lean broke the build, ask model to fix it
        for repair_step in range(10):
            out = run_lake_build(self.spec_pkg_root)
            if out.startswith("Build Success"):
                break
            log(f"Spec repair step {repair_step + 1}: build failed, asking model to fix Verif.lean")

            errs = parse_lean_errors(out, max_n=6)
            err_lines = "\n".join([f"{e.file}:{e.line}:{e.col}: {e.msg}" for e in errs])
            verif_content = _read_text_file(self.spec_src_root / out_rel) if (self.spec_src_root / out_rel).exists() else ""

            repair_payload = (
                "TASK: Fix the Verif.lean file so the project builds.\n"
                "The specification you wrote has errors. Fix them while keeping meaningful theorems.\n\n"
                f"TARGET: {out_rel}\n\n"
                "CURRENT CONTENT:\n"
                f"{trunc(verif_content, 8000)}\n\n"
                "BUILD ERRORS:\n"
                f"{trunc(err_lines, 3000)}\n\n"
                "FULL BUILD OUTPUT:\n"
                f"{trunc(out, 3000)}\n"
            )

            repair_ok = self._session_until_successful_write(
                stage="SPECIFICATION (repair)",
                focus_src="(project)",
                focus_out=out_rel,
                user_payload=repair_payload,
                max_turns=8,
            )
            if not repair_ok:
                log("Spec repair session stalled; retrying...")
        else:
            raise RuntimeError("Specification stage: Verif.lean could not be repaired after 10 attempts.")

        log("Specification complete: typechecks.")

    # ---------------- Hardening stage ----------------

    def run_stage_hardening(self) -> None:
        log("--- Stage: Hardening (Safety Case + Improvements) ---")
        self._current_stage = "HARDENING"

        instructions = self._base_instructions(stage="HARDENING")

        last_rep = self._equiv_state.get("last_report")
        rep_str = json.dumps(last_rep, indent=2) if isinstance(last_rep, dict) else str(last_rep)

        project_lean_files = sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/") and p.endswith(".lean")])
        sample_files = _limit_lines(project_lean_files, 8)
        samples: List[str] = []
        for rel in sample_files:
            p = self.spec_src_root / rel
            if p.exists():
                samples.append(f"FILE: {rel}\n" + trunc(_read_text_file(p), 2400))

        harness_lean = _read_text_file(self.spec_src_root / "tests/Harness.lean") if (self.spec_src_root / "tests/Harness.lean").exists() else ""
        gen_py = _read_text_file(Path("spec/tests/gen_inputs.py")) if Path("spec/tests/gen_inputs.py").exists() else ""
        c_h = _read_text_file(Path("spec/tests/harness.c")) if Path("spec/tests/harness.c").exists() else ""

        payload = (
            "TASK: HARDEN the result for safety-critical confidence.\n\n"
            "Step A (required): Write an expert-facing Safety Case markdown file at:\n"
            f"- {self.safety_case_rel}\n"
            "It must argue (to a formal methods expert) why your translation, differential tests, and spec are robust.\n"
            "It must include:\n"
            "- Explicit claims (translation correctness, test adequacy, harness determinism, spec relevance)\n"
            "- Concrete evidence from the differential test report (seeds, case counts)\n"
            "- Identified weaknesses/risks + mitigation plan\n\n"
            "Step B (required): ACT on the weaknesses you identified.\n"
            "- If tests are weak, improve gen_inputs.py and harnesses.\n"
            "- If translation seems brittle, improve Lean code.\n"
            "- If spec is generic, improve Verif.lean.\n"
            "- After changes, call run_differential_test and ensure success.\n\n"
            "You cannot finish by calling submit_stage unless:\n"
            "- safety case file exists\n"
            "- lake build succeeds\n"
            "- run_differential_test returns status=success with required runs and case counts\n\n"
            "CURRENT TEST REPORT:\n"
            f"{rep_str}\n\n"
            "CURRENT HARNESS FILES (snippets):\n"
            "----- spec/tests/gen_inputs.py -----\n"
            f"{trunc(gen_py, 2500)}\n\n"
            "----- spec/tests/harness.c -----\n"
            f"{trunc(c_h, 2500)}\n\n"
            "----- spec/Spec/tests/Harness.lean -----\n"
            f"{trunc(harness_lean, 3000)}\n\n"
            "SAMPLE PROJECT LEAN FILES:\n"
            + "\n\n".join(samples)
        )

        previous_response_id: Optional[str] = None
        current_input: Any = payload

        for turn in range(80):
            log(f"Turn {turn+1}")
            resp = self._responses_create(
                instructions=instructions,
                input_data=current_input,
                previous_response_id=previous_response_id,
                tool_choice=None,
                parallel_tool_calls=False,
            )
            previous_response_id = resp.id

            tool_calls = []
            if getattr(resp, "output", None):
                for item in resp.output:
                    if item.type == "message":
                        for part in item.content:
                            if part.type == "output_text":
                                log(f"Model: {trunc(part.text)}")
                    elif item.type == "function_call":
                        tool_calls.append(item)

            if not tool_calls:
                raise RuntimeError("Hardening stage stalled: no tool calls.")

            tool_outputs: List[Dict[str, Any]] = []
            submit_ok = False

            for call in tool_calls:
                out_item, ok = self._execute_tool_call(call)
                tool_outputs.append(out_item)
                if call.name == "submit_stage" and ok:
                    submit_ok = True

            current_input = tool_outputs

            if submit_ok:
                break

        if not Path(self.safety_case_rel).exists():
            raise RuntimeError("Hardening ended but safety case file missing.")

        out = run_lake_build(self.spec_pkg_root)
        if not out.startswith("Build Success"):
            raise RuntimeError("Hardening ended but build fails.")

        if self._equiv_state.get("last_status") != "success" or self._equiv_state.get("passed_runs", 0) < DIFF_REQUIRED_RUNS:
            raise RuntimeError("Hardening ended but robust differential tests not passing.")

        log("Hardening complete: safety case written and outputs improved, tests pass robustly.")

    # ---------------- Aristotle stage ----------------

    def run_stage_verification(self) -> None:
        target_file = self.spec_project_root / "Verif.lean"
        if not target_file.exists():
            log("No Verif.lean found. Skipping Aristotle.")
            return
        if not aristotlelib:
            log("aristotlelib missing. Skipping Aristotle.")
            return

        log("=== Aristotle Verification ===")
        os.environ["ARISTOTLE_API_KEY"] = self.secrets["secrets"].get("ARISTOTLE_API_KEY", "")

        try:
            cwd = os.getcwd()
            os.chdir(self.spec_pkg_root)

            rel_target = target_file.relative_to(self.spec_pkg_root)
            log(f"Submitting {rel_target} to Aristotle...")

            result = asyncio.run(
                aristotlelib.Project.prove_from_file(
                    input_file_path=str(rel_target),
                    auto_add_imports=True,
                    validate_lean_project=True,
                    wait_for_completion=True,
                )
            )

            os.chdir(cwd)
            log(f"Aristotle Output: {result}")

            res_path = self.spec_pkg_root / result
            if res_path.exists():
                res_path.rename(target_file)
                log("Verified spec saved over Verif.lean.")
                bres = run_lake_build(self.spec_pkg_root)
                log(f"Final Build: {bres}")

        except Exception as e:
            log(f"Aristotle Error: {e}")
            try:
                os.chdir(cwd)
            except Exception:
                pass

    # ---------------- Main runner ----------------

    def run(self) -> None:
        global SKIP_TO_EQUIVALENCE, SKIP_TO_SPECIFICATION
        log(f"=== Processing Project: {self.name} ===")
        self._autogen_scaffold_and_lockdown()

        if not self.src_to_lean:
            log("No source mapping; skipping.")
            return


        restart_reason: Optional[str] = None  # Passed to Translation on restart
        while True:  # Outer loop for Kickback Mechanism (from any stage)
            try:
                if not SKIP_TO_EQUIVALENCE and not SKIP_TO_HARDENING and not SKIP_TO_VERIFICATION and not SKIP_TO_SPECIFICATION:
                    self.run_stage_translation(restart_reason=restart_reason)
                elif SKIP_TO_VERIFICATION:
                    log("Skipping Translation stage (SKIP_TO_VERIFICATION=True).")
                elif SKIP_TO_HARDENING:
                    log("Skipping Translation stage (SKIP_TO_HARDENING=True).")
                elif SKIP_TO_SPECIFICATION:
                    log("Skipping Translation stage (SKIP_TO_SPECIFICATION=True).")
                else:
                    log("Skipping Translation stage (SKIP_TO_EQUIVALENCE=True).")

                if not SKIP_TO_HARDENING and not SKIP_TO_VERIFICATION and not SKIP_TO_SPECIFICATION:
                    self.run_stage_equivalence()
                else:
                    log("Skipping Equivalence stage.")

                if not SKIP_TO_VERIFICATION:
                    self.run_stage_specification()
                    self.run_stage_hardening()
                else:
                    log("Skipping Spec & Hardening stages (SKIP_TO_VERIFICATION=True).")

                self.run_stage_verification()

                # If we get here, all stages passed. Exit the loop.
                break

            except RestartTranslationError as e:
                log(f"\n!!! KICKBACK TRIGGERED: {e.reason} !!!")
                log("Restarting Translation Stage with fresh context...\n")
                restart_reason = e.reason  # Pass reason to next Translation iteration
                # Ensure we will actually translate on the next loop.
                if SKIP_TO_EQUIVALENCE or SKIP_TO_HARDENING or SKIP_TO_VERIFICATION or SKIP_TO_SPECIFICATION:
                    log("WARNING: Kickback triggered but SKIP flags are active. Disabling all SKIP flags to allow re-translation.")
                    SKIP_TO_EQUIVALENCE = False
                    SKIP_TO_SPECIFICATION = False
                    # Keep SKIP_TO_HARDENING and SKIP_TO_VERIFICATION as-is since they're more drastic
                continue


# ============================================================
# Main
# ============================================================

def main() -> None:
    log("=== Anneal Universal Verification Agent ===")
    secrets = load_secrets()
    client = OpenAI(api_key=secrets["secrets"]["OPENAI_API_KEY"])

    if not EXAMPLES_DIR.exists():
        log(f"No source projects found in {EXAMPLES_DIR}")
        return

    examples = [d for d in EXAMPLES_DIR.iterdir() if d.is_dir()]
    if not examples:
        log("No source projects found.")
        return

    for ex in examples:
        proc = ProjectProcessor(ex.name, ex, client, secrets)
        proc.run()


if __name__ == "__main__":
    main()
