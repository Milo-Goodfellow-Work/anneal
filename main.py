#!/usr/bin/env python3
from __future__ import annotations

import os
import re
import sys
import time
import json
import tomllib
import shutil
import asyncio
import subprocess
from pathlib import Path
from typing import List, Optional, Dict, Tuple, Any, Set
from dataclasses import dataclass

from openai import OpenAI

try:
    import aristotlelib
except ImportError:
    aristotlelib = None


# ============================================================
# Configuration
# ============================================================

SECRETS_FILE = Path("secrets.toml")

SPEC_DIR = Path("spec")
SPEC_SRC_DIR = SPEC_DIR / "Spec"
EXAMPLES_DIR = Path("examples")

MODEL_ID = "gpt-5.2"

PRINT_TRUNC = 4000
MAX_TOOL_READ_CHARS = 80_000
MAX_REPAIR_TURNS = 12
MAX_SESSION_TURNS = 12

# Autogenerated, model-writable artifacts outside Spec/Spec
SPEC_TESTS_DIR = SPEC_DIR / "tests"

# Files that aggregate imports / define the environment surface.
# We will auto-generate and keep them read-only for the model.
LOCKED_LEAN_FILENAMES = {"Prelude.lean"}  # plus per-project root module "<project>.lean" added dynamically

# Prelude policy: we own these imports; generated files should only import Spec.Prelude (and optionally Spec.<project>.*)
PRELUDE_REQUIRED_IMPORTS = [
    "Std",
    "Std.Data.TreeMap",
    "Std.Data.TreeSet",
    "Std.Data.HashMap",
    "Std.Data.HashSet",
]


# ============================================================
# Utilities
# ============================================================

def log(msg: str) -> None:
    print(f"[Anneal] {msg}", flush=True)

def trunc(s: str, n: int = PRINT_TRUNC) -> str:
    if s is None:
        return ""
    return s if len(s) <= n else (s[:n] + f"\n... (truncated, total {len(s)} chars)")

def load_secrets() -> dict:
    if not SECRETS_FILE.exists():
        key = os.environ.get("OPENAI_API_KEY")
        if key:
            aristotle_key = os.environ.get("ARISTOTLE_API_KEY")
            secrets = {"OPENAI_API_KEY": key}
            if aristotle_key:
                secrets["ARISTOTLE_API_KEY"] = aristotle_key
            return {"secrets": secrets}
        raise FileNotFoundError(f"{SECRETS_FILE} not found and OPENAI_API_KEY not in env.")
    with SECRETS_FILE.open("rb") as f:
        return tomllib.load(f)

def _safe_relpath(p: str) -> str:
    p = p.replace("\\", "/").lstrip("/")
    if p == "" or p == ".":
        raise ValueError("Empty path")
    parts = [x for x in p.split("/") if x not in ("", ".")]
    if any(x == ".." for x in parts):
        raise ValueError(f"Path traversal not allowed: {p}")
    return "/".join(parts)

def _read_text_file(p: Path) -> str:
    return p.read_text(encoding="utf-8", errors="replace")

def _write_text_file(p: Path, content: str) -> None:
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(content, encoding="utf-8")

def list_project_files(base_dir: Path) -> List[str]:
    files: List[str] = []
    if not base_dir.exists():
        return files
    for root, dirs, filenames in os.walk(base_dir):
        if ".git" in dirs:
            dirs.remove(".git")
        if "__pycache__" in dirs:
            dirs.remove("__pycache__")
        for fn in filenames:
            abs_p = Path(root) / fn
            rel_p = abs_p.relative_to(base_dir)
            files.append(str(rel_p).replace("\\", "/"))
    return sorted(files)

def list_lean_files(base_dir: Path) -> List[str]:
    files: List[str] = []
    if not base_dir.exists():
        return files
    for root, dirs, filenames in os.walk(base_dir):
        if ".git" in dirs:
            dirs.remove(".git")
        if "__pycache__" in dirs:
            dirs.remove("__pycache__")
        for fn in filenames:
            if fn.endswith(".lean"):
                abs_p = Path(root) / fn
                rel_p = abs_p.relative_to(base_dir)
                files.append(str(rel_p).replace("\\", "/"))
    return sorted(files)

def run_lake_build(cwd: Path) -> str:
    start = time.time()
    try:
        res = subprocess.run(
            ["lake", "build"],
            cwd=str(cwd),
            capture_output=True,
            text=True,
            check=False,
        )
        elapsed = time.time() - start
        if res.returncode == 0:
            return f"Build Success ({elapsed:.2f}s)"
        return f"Build Failed (exit={res.returncode}, {elapsed:.2f}s):\n{res.stderr}\n{res.stdout}"
    except Exception as e:
        return f"Error running lake build: {e}"

def run_lake_build_target(cwd: Path, target: Optional[str] = None) -> str:
    start = time.time()
    cmd = ["lake", "build"]
    if target:
        cmd.append(target)
    try:
        res = subprocess.run(cmd, cwd=str(cwd), capture_output=True, text=True, check=False)
        elapsed = time.time() - start
        if res.returncode == 0:
            return f"Build Success ({elapsed:.2f}s)"
        return f"Build Failed (exit={res.returncode}, {elapsed:.2f}s):\n{res.stderr}\n{res.stdout}"
    except Exception as e:
        return f"Error running lake build: {e}"

def module_name_from_lean_path(rel_path_under_spec: str) -> Optional[str]:
    p = Path(rel_path_under_spec)
    if p.suffix != ".lean":
        return None
    return "Spec." + ".".join(p.with_suffix("").parts)

def _slug_to_camel(s: str) -> str:
    parts = re.split(r"[^A-Za-z0-9]+", s)
    parts = [x for x in parts if x]
    if not parts:
        return "X"
    out = "".join(x[:1].upper() + x[1:] for x in parts)
    if out and out[0].isdigit():
        out = "X" + out
    return out

def _lean_out_path_for_source(project: str, src_rel: str, used: Dict[str, int]) -> str:
    src = Path(src_rel)
    stem = src.stem
    parent = str(src.parent).replace("\\", "/")
    if parent in (".", ""):
        base = _slug_to_camel(stem)
    else:
        base = _slug_to_camel(parent.replace("/", "_") + "_" + stem)
    name = base
    if name in used:
        used[name] += 1
        name = f"{name}{used[name]}"
    else:
        used[name] = 1
    return f"{project}/{name}.lean"

def _is_source_file(rel: str) -> bool:
    ext = Path(rel).suffix.lower()
    return ext in {".c", ".h", ".cc", ".cpp", ".hpp"}

def _limit_lines(lines: List[str], max_lines: int = 200) -> List[str]:
    if len(lines) <= max_lines:
        return lines
    return lines[:max_lines] + [f"... ({len(lines)-max_lines} more lines omitted) ..."]


# ============================================================
# Lean error parsing (for targeted repair)
# ============================================================

LEAN_ERR_RE = re.compile(
    r"^error:\s+(?P<file>[^:]+):(?P<line>\d+):(?P<col>\d+):\s+(?P<msg>.*)$",
    re.MULTILINE,
)

@dataclass
class LeanError:
    file: str
    line: int
    col: int
    msg: str

def parse_lean_errors(build_output: str, *, max_n: int = 5) -> List[LeanError]:
    errs: List[LeanError] = []
    for m in LEAN_ERR_RE.finditer(build_output):
        errs.append(
            LeanError(
                file=m.group("file").strip(),
                line=int(m.group("line")),
                col=int(m.group("col")),
                msg=m.group("msg").strip(),
            )
        )
        if len(errs) >= max_n:
            break
    return errs

def excerpt_around(text: str, line_1based: int, *, radius: int = 12) -> str:
    lines = text.splitlines()
    i = max(1, line_1based) - 1
    lo = max(0, i - radius)
    hi = min(len(lines), i + radius + 1)
    out: List[str] = []
    for idx in range(lo, hi):
        prefix = ">>" if idx == i else "  "
        out.append(f"{prefix} {idx+1:4d}: {lines[idx]}")
    return "\n".join(out)


# ============================================================
# Prelude + locked files
# ============================================================

PRELUDE_PATH = SPEC_SRC_DIR / "Prelude.lean"

def ensure_prelude_and_lockdown() -> None:
    """
    Ensure Prelude exists and contains a conservative, useful import surface.
    Model will be prevented from writing it; the script owns it.
    If Prelude exists, we will only prepend missing required imports.
    """
    SPEC_SRC_DIR.mkdir(parents=True, exist_ok=True)

    required_import_lines = [f"import {m}" for m in PRELUDE_REQUIRED_IMPORTS]

    if not PRELUDE_PATH.exists():
        content = (
            "\n".join(required_import_lines)
            + "\n\nnamespace Spec\n\n"
              "-- Shared abbreviations and safe utilities live here.\n"
              "-- Generated modules must import ONLY Spec.Prelude (plus Spec.<project>.* if needed).\n\n"
              "abbrev U8  := UInt8\n"
              "abbrev U16 := UInt16\n"
              "abbrev U32 := UInt32\n"
              "abbrev U64 := UInt64\n\n"
              "end Spec\n"
        )
        _write_text_file(PRELUDE_PATH, content)
        log(f"Wrote {PRELUDE_PATH}")
        return

    # If it exists, minimally ensure required imports exist (prepend missing ones).
    existing = _read_text_file(PRELUDE_PATH)
    existing_imports = set(re.findall(r"^\s*import\s+([A-Za-z0-9_.]+)\s*$", existing, flags=re.MULTILINE))
    missing = [m for m in PRELUDE_REQUIRED_IMPORTS if m not in existing_imports]
    if missing:
        prepend = "\n".join([f"import {m}" for m in missing]) + "\n"
        _write_text_file(PRELUDE_PATH, prepend + existing)
        log(f"Updated {PRELUDE_PATH}: prepended missing imports: {missing}")


# ============================================================
# Tool schema
# ============================================================

def _tool(name: str, description: str, properties: Dict[str, Any], required: List[str]) -> Dict[str, Any]:
    return {
        "type": "function",
        "name": name,
        "description": description,
        "strict": True,
        "parameters": {
            "type": "object",
            "properties": properties,
            "required": required,
            "additionalProperties": False,
        },
    }

TOOLS_SCHEMA = [
    _tool(
        "read_source_file",
        "Read content of a source file from examples/<project>/ (relative path).",
        {"path": {"type": "string", "description": "Relative path under examples/<project>/"}},
        ["path"],
    ),
    _tool(
        "read_lean_file",
        "Read content of a Lean file from spec/Spec/ (relative path).",
        {"path": {"type": "string", "description": "Relative path under spec/Spec/ (e.g., 'order_engine/Engine.lean')"}},
        ["path"],
    ),
    _tool(
        "write_lean_file",
        "Write or overwrite a Lean file under spec/Spec/ (relative path). Writes are restricted to pre-autogenerated files only.",
        {
            "path": {"type": "string", "description": "Relative path under spec/Spec/"},
            "content": {"type": "string", "description": "Full content of the Lean file"},
        },
        ["path", "content"],
    ),
    _tool(
        "write_text_file",
        "Write or overwrite a non-Lean file. Writes are restricted to pre-autogenerated files only.",
        {
            "path": {"type": "string", "description": "Relative path under the repository root (e.g., 'spec/tests/harness.c')"},
            "content": {"type": "string", "description": "Full content of the file"},
        },
        ["path", "content"],
    ),
    _tool(
        "verify_build",
        "Run lake build in the spec/ directory.",
        {},
        [],
    ),
    _tool(
        "run_differential_test",
        "Compile and run C and Lean harnesses on generated inputs, comparing stdout.",
        {
            "gen_script_path": {"type": "string", "description": "Path to python input generator (relative to repo root)"},
            "c_harness_path": {"type": "string", "description": "Path to C harness source (relative to repo root)"},
            "lean_harness_path": {"type": "string", "description": "Path to Lean harness source (relative to spec/Spec/)"},
        },
        ["gen_script_path", "c_harness_path", "lean_harness_path"],
    ),
    _tool(
        "submit_stage",
        "Signal that the current stage is complete.",
        {"summary": {"type": "string", "description": "Brief summary"}},
        ["summary"],
    ),
]


# ============================================================
# Import policy for generated Lean files (model-facing)
# ============================================================

IMPORT_RE = re.compile(r"^\s*import\s+([A-Za-z0-9_.]+)\s*$", re.MULTILINE)

def validate_imports_for_project(project: str, content: str) -> Tuple[bool, List[str]]:
    """
    Policy:
    - Must import Spec.Prelude at least once.
    - May additionally import Spec.<project>.* (rarely needed).
    - No Std / Mathlib / anything else in generated files.
    """
    mods = IMPORT_RE.findall(content)
    bad: List[str] = []
    for mod in mods:
        if mod == "Spec.Prelude":
            continue
        if mod.startswith(f"Spec.{project}."):
            continue
        # Allow importing the project root module itself if needed
        if mod == f"Spec.{project}":
            continue
        bad.append(mod)

    if "Spec.Prelude" not in mods:
        bad.append("(missing Spec.Prelude)")

    return (len(bad) == 0, bad)

def validate_basic_lean_shape(project: str, content: str) -> Tuple[bool, str]:
    stripped = content.strip()
    if not stripped:
        return False, "File content is empty or whitespace."
    if "namespace" not in content or f"namespace Spec.{project}" not in content:
        return False, f"Missing required namespace 'namespace Spec.{project}'."
    if f"end Spec.{project}" not in content:
        return False, f"Missing required end marker 'end Spec.{project}'."
    ok_imports, bad = validate_imports_for_project(project, content)
    if not ok_imports:
        return False, f"Import policy violated: {bad}. Only 'import Spec.Prelude' (and optionally Spec.{project}.*) allowed."
    return True, ""


# ============================================================
# Processor
# ============================================================

class ProjectProcessor:
    def __init__(self, example_name: str, example_path: Path, client: OpenAI, secrets: dict):
        self.name = example_name
        self.source_root = example_path

        self.spec_pkg_root = SPEC_DIR
        self.spec_src_root = SPEC_SRC_DIR
        self.spec_project_root = self.spec_src_root / example_name  # spec/Spec/<project>/

        self.client = client
        self.secrets = secrets

        self.spec_project_root.mkdir(parents=True, exist_ok=True)

        # Autogenerated write-allowlists
        self.allowed_lean_writes: Set[str] = set()   # paths relative to spec/Spec
        self.allowed_text_writes: Set[str] = set()   # paths relative to repo root
        self.locked_lean_paths: Set[str] = set(LOCKED_LEAN_FILENAMES)

        # Source->Lean mapping
        self.src_to_lean: Dict[str, str] = {}
        self.lean_to_src: Dict[str, str] = {}

    # ---------------- OpenAI helpers ----------------

    def _responses_create(
        self,
        *,
        instructions: str,
        input_data: Any,
        previous_response_id: Optional[str] = None,
        tool_choice: Optional[Any] = None,
        parallel_tool_calls: bool = False,
    ):
        kwargs: Dict[str, Any] = {
            "model": MODEL_ID,
            "instructions": instructions,
            "input": input_data,
            "tools": TOOLS_SCHEMA,
            "parallel_tool_calls": parallel_tool_calls,
        }
        if previous_response_id:
            kwargs["previous_response_id"] = previous_response_id
        if tool_choice is not None:
            kwargs["tool_choice"] = tool_choice
        return self.client.responses.create(**kwargs)

    def _tool_output_item(self, call_id: str, out: str) -> Dict[str, Any]:
        return {"type": "function_call_output", "call_id": call_id, "output": out}

    def _execute_tool_call(self, item) -> Tuple[Dict[str, Any], bool]:
        """
        Execute a single tool call from the model.
        Returns (function_call_output_item, success_flag_for_write_calls).
        """
        fname = item.name
        call_id = item.call_id
        try:
            args = json.loads(item.arguments) if item.arguments else {}
        except json.JSONDecodeError:
            args = {}

        # log args (hide content)
        log_args = dict(args)
        if "content" in log_args and isinstance(log_args["content"], str):
            log_args["content"] = f"<{len(log_args['content'])} chars>"
        log(f"Call: {fname}({json.dumps(log_args)})")

        try:
            if fname == "read_source_file":
                rel = _safe_relpath(args["path"])
                p = self.source_root / rel
                if p.exists() and p.is_file():
                    out = _read_text_file(p)
                    if len(out) > MAX_TOOL_READ_CHARS:
                        out = out[:MAX_TOOL_READ_CHARS] + f"\n\n-- TRUNCATED at {MAX_TOOL_READ_CHARS} chars --"
                    return self._tool_output_item(call_id, out), True
                if p.exists() and p.is_dir():
                    return self._tool_output_item(call_id, f"Error: {rel} is a directory."), True
                return self._tool_output_item(call_id, f"Error: File not found {rel}"), True

            if fname == "read_lean_file":
                rel = _safe_relpath(args["path"])
                p = self.spec_src_root / rel
                if p.exists() and p.is_file():
                    out = _read_text_file(p)
                    if len(out) > MAX_TOOL_READ_CHARS:
                        out = out[:MAX_TOOL_READ_CHARS] + f"\n\n-- TRUNCATED at {MAX_TOOL_READ_CHARS} chars --"
                    return self._tool_output_item(call_id, out), True
                if p.exists() and p.is_dir():
                    return self._tool_output_item(call_id, f"Error: {rel} is a directory."), True
                return self._tool_output_item(call_id, f"Error: Lean file not found {rel}"), True

            if fname == "write_lean_file":
                rel = _safe_relpath(args["path"])

                # Enforce file-level lockdown
                if rel in self.locked_lean_paths:
                    msg = (
                        f"Denied: '{rel}' is locked (read-only). "
                        f"Locked: {sorted(self.locked_lean_paths)}"
                    )
                    return self._tool_output_item(call_id, msg), False

                # Enforce allowlist
                if rel not in self.allowed_lean_writes:
                    msg = (
                        f"Denied: '{rel}' is not in the autogenerated writable set. "
                        f"You may only write these Lean files:\n"
                        + "\n".join(sorted(self.allowed_lean_writes))
                    )
                    return self._tool_output_item(call_id, msg), False

                content = args.get("content", "")
                ok, reason = validate_basic_lean_shape(self.name, content)
                if not ok:
                    return self._tool_output_item(call_id, f"Rejected content for '{rel}': {reason}"), False

                p = self.spec_src_root / rel
                _write_text_file(p, content)
                return self._tool_output_item(call_id, f"Written to {p}"), True

            if fname == "write_text_file":
                rel = _safe_relpath(args["path"])

                # Enforce allowlist
                if rel not in self.allowed_text_writes:
                    msg = (
                        f"Denied: '{rel}' is not in the autogenerated writable set. "
                        f"You may only write these files:\n"
                        + "\n".join(sorted(self.allowed_text_writes))
                    )
                    return self._tool_output_item(call_id, msg), False

                p = Path(rel)
                content = args.get("content", "")
                if not content.strip():
                    return self._tool_output_item(call_id, f"Rejected: '{rel}' content is empty."), False

                _write_text_file(p, content)
                return self._tool_output_item(call_id, f"Written to {p}"), True

            if fname == "verify_build":
                out = run_lake_build(self.spec_pkg_root)
                return self._tool_output_item(call_id, out), True

            if fname == "run_differential_test":
                out = self._run_differential_test_impl(args)
                return self._tool_output_item(call_id, out), True

            if fname == "submit_stage":
                out = f"Stage Submitted: {args.get('summary','')}"
                log(out)
                return self._tool_output_item(call_id, out), True

            return self._tool_output_item(call_id, f"Error: Unknown tool {fname}"), True

        except Exception as e:
            return self._tool_output_item(call_id, f"Tool execution error for {fname}: {e}"), False

    def _run_differential_test_impl(self, args: Dict[str, Any]) -> str:
        """
        Runs a simple differential test:
        - python gen script outputs stdin text
        - run compiled C harness with stdin, capture stdout
        - run Lean harness via lake env lean --run with same stdin, capture stdout
        Compare stdout.
        """
        gen_script = _safe_relpath(args.get("gen_script_path", "spec/tests/gen_inputs.py"))
        c_harness = _safe_relpath(args.get("c_harness_path", "spec/tests/harness.c"))
        lean_harness = _safe_relpath(args.get("lean_harness_path", "tests/Harness.lean"))  # relative to spec/Spec

        # Compile C harness
        exe_c = SPEC_TESTS_DIR / "harness.exe"
        c_harness_abs = Path(c_harness)
        if not c_harness_abs.exists():
            return f"C harness not found: {c_harness}"
        if not Path(gen_script).exists():
            return f"Generator script not found: {gen_script}"

        c_srcs = [
            str(self.source_root / f)
            for f in list_project_files(self.source_root)
            if f.endswith(".c") and "main.c" not in f.replace("\\", "/").lower()
        ]
        c_cmd = ["gcc", "-O2", "-o", str(exe_c), str(c_harness_abs)] + c_srcs
        proc_c = subprocess.run(c_cmd, capture_output=True, text=True)
        if proc_c.returncode != 0:
            return f"C Compile Failed:\n{proc_c.stderr}\n{proc_c.stdout}"

        # Ensure Lean builds (baseline)
        b_out = run_lake_build(self.spec_pkg_root)
        if "Build Failed" in b_out:
            return f"Lean Build Failed:\n{b_out}"

        # Generate inputs
        try:
            gen_proc = subprocess.run([sys.executable, gen_script], capture_output=True, text=True, check=True)
            inputs = gen_proc.stdout
        except subprocess.CalledProcessError as e:
            return f"Input Generation Failed:\n{e.stderr}\n{e.stdout}"

        # Run C
        try:
            c_run = subprocess.run([str(exe_c)], input=inputs, capture_output=True, text=True, timeout=5)
            c_out = c_run.stdout
        except Exception as e:
            return f"C Runtime Error: {e}"

        # Run Lean
        lean_run_path = self.spec_src_root / lean_harness
        if not lean_run_path.exists():
            return f"Lean harness not found: spec/Spec/{lean_harness}"

        try:
            lean_cmd = ["lake", "env", "lean", "--run", str(lean_run_path)]
            lean_run = subprocess.run(
                lean_cmd,
                cwd=str(self.spec_pkg_root),
                input=inputs,
                capture_output=True,
                text=True,
                timeout=10,
            )
            lean_out = lean_run.stdout
            if lean_run.returncode != 0:
                lean_out += f"\n[Exit {lean_run.returncode}]\n{lean_run.stderr}"
        except Exception as e:
            return f"Lean Runtime Error: {e}"

        if c_out == lean_out:
            return "SUCCESS: Outputs match."
        return (
            "DIFF DETECTED\n"
            "----- C OUTPUT (truncated) -----\n"
            f"{trunc(c_out, 1200)}\n"
            "----- LEAN OUTPUT (truncated) -----\n"
            f"{trunc(lean_out, 1200)}\n"
        )

    # ---------------- Structure generation and lockdown ----------------

    def _register_module_in_package_root(self) -> None:
        """
        Ensure spec/Spec.lean imports Spec.<project>.
        This file is outside spec/Spec/, so the model cannot write it anyway.
        """
        spec_file = self.spec_src_root.parent / "Spec.lean"  # spec/Spec.lean
        line = f"import Spec.{self.name}"
        if spec_file.exists():
            content = _read_text_file(spec_file)
            if line not in content:
                _write_text_file(spec_file, content.rstrip() + "\n" + line + "\n")
        else:
            _write_text_file(spec_file, line + "\n")

    def _write_project_root_module_locked(self, project_module_paths: List[str]) -> None:
        """
        Write spec/Spec/<project>.lean importing all project submodules.
        Model will be forbidden from writing this file.
        """
        imports: List[str] = []
        for rel in project_module_paths:
            p = Path(rel)
            if p.suffix != ".lean":
                continue
            mod = "Spec." + ".".join(p.with_suffix("").parts)
            imports.append(f"import {mod}")

        root_rel = f"{self.name}.lean"
        body = "\n".join(sorted(set(imports))) + "\n"
        _write_text_file(self.spec_src_root / root_rel, body)
        self.locked_lean_paths.add(root_rel)

    def _autogen_scaffold_and_lockdown(self) -> None:
        """
        Auto-generate the Lean file structure inside spec/Spec and lock down writes so the model
        can only write to those autogenerated files. Also autogenerate test harness paths and allow
        writing only to them (not arbitrary files).
        """
        # Ensure Prelude exists and is script-owned
        ensure_prelude_and_lockdown()

        # Discover source files
        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        if not src_files:
            log(f"No C/C++ source files found under {self.source_root}")
            return

        used_names: Dict[str, int] = {}
        project_module_paths: List[str] = []

        # Build stable mapping
        self.src_to_lean.clear()
        self.lean_to_src.clear()

        for rel in src_files:
            out_rel = _lean_out_path_for_source(self.name, rel, used_names)
            self.src_to_lean[rel] = out_rel
            self.lean_to_src[out_rel] = rel
            project_module_paths.append(out_rel)

        # Autogenerate stub modules (model-writable)
        self.allowed_lean_writes = set(project_module_paths)

        for out_rel in project_module_paths:
            p = self.spec_src_root / out_rel
            if not p.exists():
                stub = (
                    "import Spec.Prelude\n\n"
                    f"namespace Spec.{self.name}\n\n"
                    f"-- AUTOGENERATED STUB\n"
                    f"-- Source: {self.lean_to_src.get(out_rel, '(unknown)')}\n"
                    f"-- The translation agent must replace this stub with actual definitions.\n\n"
                    f"end Spec.{self.name}\n"
                )
                _write_text_file(p, stub)

        # Autogenerate a Verif stub (model-writable) now so it's in allowlist
        verif_rel = f"{self.name}/Verif.lean"
        self.allowed_lean_writes.add(verif_rel)
        if not (self.spec_src_root / verif_rel).exists():
            verif_stub = (
                "import Spec.Prelude\n\n"
                f"namespace Spec.{self.name}\n\n"
                "-- AUTOGENERATED SPEC STUB\n"
                "-- Fill this file with theorems and invariants (proofs may use sorry).\n\n"
                f"end Spec.{self.name}\n"
            )
            _write_text_file(self.spec_src_root / verif_rel, verif_stub)

        # Autogenerate Lean harness placeholder under spec/Spec/tests/Harness.lean (optional)
        harness_rel = "tests/Harness.lean"
        self.allowed_lean_writes.add(harness_rel)
        harness_path = self.spec_src_root / harness_rel
        if not harness_path.exists():
            harness_stub = (
                "import Spec.Prelude\n"
                f"import Spec.{self.name}\n\n"
                f"namespace Spec.{self.name}\n\n"
                "-- AUTOGENERATED HARNESS STUB\n"
                "-- This module should parse stdin commands and print results for differential testing.\n\n"
                "def main : IO Unit := do\n"
                "  IO.println \"Harness not implemented\"\n\n"
                f"end Spec.{self.name}\n"
            )
            _write_text_file(harness_path, harness_stub)

        # Autogenerate non-Lean harness files in spec/tests (model-writable)
        SPEC_TESTS_DIR.mkdir(parents=True, exist_ok=True)
        gen_rel = "spec/tests/gen_inputs.py"
        c_rel = "spec/tests/harness.c"
        self.allowed_text_writes = {gen_rel, c_rel}

        if not Path(gen_rel).exists():
            _write_text_file(Path(gen_rel),
                "#!/usr/bin/env python3\n"
                "import random\n"
                "\n"
                "# AUTOGENERATED STUB\n"
                "# Print test cases to stdout. The C and Lean harnesses must consume the same format.\n"
                "\n"
                "def main():\n"
                "    # Example: print no inputs\n"
                "    print(\"\")\n"
                "\n"
                "if __name__ == '__main__':\n"
                "    main()\n"
            )

        if not Path(c_rel).exists():
            _write_text_file(Path(c_rel),
                "/* AUTOGENERATED STUB HARNESS\n"
                " * Read stdin inputs, call into the project, print outputs.\n"
                " */\n"
                "#include <stdio.h>\n"
                "\n"
                "int main(void) {\n"
                "    // TODO: implement\n"
                "    return 0;\n"
                "}\n"
            )

        # Write project root module (locked)
        self._write_project_root_module_locked(project_module_paths)

        # Ensure package root imports project
        self._register_module_in_package_root()

        # Lock Prelude and project root module
        self.locked_lean_paths.add("Prelude.lean")
        self.locked_lean_paths.add(f"{self.name}.lean")

        log("Autogenerated Spec/Spec scaffold and locked down writes.")
        log(f"Locked Lean files (read-only for model): {sorted(self.locked_lean_paths)}")
        log(f"Writable Lean files (model may edit): {len(self.allowed_lean_writes)} files")
        log(f"Writable text files (model may edit): {sorted(self.allowed_text_writes)}")

    # ---------------- Context + prompting ----------------

    def _spec_layout_for_prompt(self) -> str:
        """
        Show a stable subset of layout to keep prompts short:
        - Prelude
        - project root module
        - project modules
        - tests/Harness.lean
        """
        shown: List[str] = []
        candidates = list_lean_files(self.spec_src_root)
        # Prioritize
        priority = []
        for p in candidates:
            if p == "Prelude.lean":
                priority.append(p)
            elif p == f"{self.name}.lean":
                priority.append(p)
            elif p.startswith(f"{self.name}/"):
                priority.append(p)
            elif p.startswith("tests/"):
                priority.append(p)
        # De-dupe while preserving order
        seen = set()
        for p in priority:
            if p not in seen:
                shown.append(p)
                seen.add(p)
        shown = _limit_lines(shown, 200)
        return "\n".join(shown)

    def _source_layout_for_prompt(self) -> str:
        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        src_files = _limit_lines(src_files, 200)
        return "\n".join(src_files)

    def _mapping_for_prompt(self) -> str:
        lines: List[str] = []
        for src, out in sorted(self.src_to_lean.items()):
            lines.append(f"{src}  ->  {out}")
        lines = _limit_lines(lines, 250)
        return "\n".join(lines)

    def _writable_files_for_prompt(self) -> str:
        lines = sorted(self.allowed_lean_writes)
        lines = _limit_lines(lines, 250)
        return "\n".join(lines)

    def _locked_files_for_prompt(self) -> str:
        lines = sorted(self.locked_lean_paths)
        return "\n".join(lines)

    def _base_instructions(self, stage: str, focus_src: Optional[str] = None, focus_out: Optional[str] = None) -> str:
        """
        These instructions are passed every single model turn.
        Keep them consistent and explicit.
        """
        focus_lines = ""
        if focus_src and focus_out:
            focus_lines = (
                f"FOCUS FILE (source -> target): {focus_src}  ->  {focus_out}\n"
                f"You must ultimately update ONLY: {focus_out}\n"
            )

        return (
            "ROLE: You are Anneal's translation engine: an expert engineer translating arbitrary input source code into Lean 4.\n"
            "OBJECTIVE: produce Lean 4 code that compiles under the existing Lake project in spec/ and matches source semantics.\n\n"
            f"STAGE: {stage}\n"
            f"PROJECT: {self.name}\n"
            f"SOURCE ROOT: {self.source_root}\n"
            f"SPEC ROOT: {SPEC_DIR}\n"
            f"SPEC SOURCE ROOT: {SPEC_SRC_DIR}\n\n"
            f"{focus_lines}"
            "IMPORTANT CONSTRAINTS (enforced by the runner):\n"
            "- You may ONLY write Lean files that were autogenerated by the runner. If you try to write any other path, it will be denied.\n"
            "- The following Lean files are LOCKED (read-only). Do not attempt to modify them:\n"
            f"{self._locked_files_for_prompt()}\n\n"
            "- Import policy for generated modules:\n"
            "  * Each generated module MUST include 'import Spec.Prelude'.\n"
            "  * Do NOT import Std or Mathlib directly inside generated modules.\n"
            "  * You may import Spec.<project>.* only if necessary.\n\n"
            "CURRENT SPEC LAYOUT (key Lean files):\n"
            f"{self._spec_layout_for_prompt()}\n\n"
            "CURRENT SOURCE LAYOUT (C/C++ files):\n"
            f"{self._source_layout_for_prompt()}\n\n"
            "SOURCE -> LEAN MAPPING (autogenerated):\n"
            f"{self._mapping_for_prompt()}\n\n"
            "LEAN FILES YOU ARE ALLOWED TO WRITE (autogenerated allowlist):\n"
            f"{self._writable_files_for_prompt()}\n\n"
            "WORKING STYLE:\n"
            "- Prefer simple, dependable data structures first (List/Array/Option/Nat/Int). Avoid inventing complex trees.\n"
            "- If the source is large, implement the core types and pure functions first; keep IO minimal.\n"
            "- When fixing build errors: make the smallest change that resolves the first error.\n"
        )

    def _project_summary(self) -> str:
        """
        Produce a brief project summary once, then include it in per-file prompts.
        """
        files = list_project_files(self.source_root)
        src_files = [f for f in files if _is_source_file(f)]
        # Include small snippets of a few files
        blobs: List[str] = []
        for rel in src_files[:8]:
            txt = _read_text_file(self.source_root / rel)
            blobs.append(f"FILE: {rel}\n" + trunc(txt, 2500) + "\n")
        instructions = (
            "Summarize this C/C++ codebase for a Lean translator.\n"
            "Return a concise, technical summary: key data structures, APIs, and control flow.\n"
        )
        resp = self._responses_create(instructions=instructions, input_data="\n".join(blobs))
        # Responses API usually has output_text; fall back safely
        summary = getattr(resp, "output_text", None)
        if not summary:
            summary = "No summary."
        return summary

    # ---------------- Session driver: allow reads; require a successful write ----------------

    def _session_until_successful_write(
        self,
        *,
        stage: str,
        focus_src: str,
        focus_out: str,
        user_payload: str,
        max_turns: int = MAX_SESSION_TURNS,
    ) -> bool:
        """
        Runs a mini interaction where the model may read files, but must eventually write focus_out.
        We stop when a write_lean_file succeeds for focus_out.
        """
        instructions = self._base_instructions(stage=stage, focus_src=focus_src, focus_out=focus_out)
        previous_response_id: Optional[str] = None
        current_input: Any = user_payload

        for turn in range(max_turns):
            log(f"Turn {turn+1}")
            resp = self._responses_create(
                instructions=instructions,
                input_data=current_input,
                previous_response_id=previous_response_id,
                tool_choice=None,
                parallel_tool_calls=False,
            )
            previous_response_id = resp.id

            tool_calls = []
            if getattr(resp, "output", None):
                for item in resp.output:
                    if item.type == "message":
                        for part in item.content:
                            if part.type == "output_text":
                                log(f"Model: {trunc(part.text)}")
                    elif item.type == "function_call":
                        tool_calls.append(item)

            if not tool_calls:
                log("No tool calls; session stalled.")
                return False

            tool_outputs: List[Dict[str, Any]] = []
            wrote_ok = False

            # Execute all tool calls (but we will detect successful write to focus_out)
            for call in tool_calls:
                out_item, ok = self._execute_tool_call(call)
                tool_outputs.append(out_item)
                if call.name == "write_lean_file":
                    # If the model wrote some other file successfully, keep going until it writes focus_out.
                    try:
                        args = json.loads(call.arguments) if call.arguments else {}
                    except json.JSONDecodeError:
                        args = {}
                    path = args.get("path", "")
                    path = path.replace("\\", "/")
                    if ok and path == focus_out:
                        wrote_ok = True

            current_input = tool_outputs

            if wrote_ok:
                return True

        log("Session exceeded max turns without a successful write to the focus file.")
        return False

    # ---------------- Translation + targeted convergence ----------------

    def _translate_file(self, *, project_summary: str, src_rel: str, out_rel: str) -> bool:
        """
        Translate one source file into its Lean target. Uses a read-allowed/write-required session.
        """
        src_txt = _read_text_file(self.source_root / src_rel)
        existing = _read_text_file(self.spec_src_root / out_rel) if (self.spec_src_root / out_rel).exists() else ""

        user_payload = (
            "TASK: Translate the given source file into the given Lean file.\n"
            "You may call read_source_file / read_lean_file if needed.\n"
            "You MUST call write_lean_file to fully overwrite the target Lean file.\n\n"
            "PROJECT SUMMARY:\n"
            f"{project_summary}\n\n"
            f"SOURCE FILE: {src_rel}\n"
            "----- BEGIN SOURCE CONTENT -----\n"
            f"{src_txt}\n"
            "----- END SOURCE CONTENT -----\n\n"
            f"TARGET LEAN FILE: {out_rel}\n"
            "----- BEGIN CURRENT TARGET CONTENT -----\n"
            f"{trunc(existing, 6000)}\n"
            "----- END CURRENT TARGET CONTENT -----\n\n"
            "REQUIREMENTS:\n"
            f"- The Lean file MUST be in namespace Spec.{self.name}\n"
            "- It MUST import Spec.Prelude\n"
            "- Keep function names recognizable (camelCase is fine).\n"
            "- Prefer simple structures first; avoid inventing complex trees/maps unless truly needed.\n"
        )

        ok = self._session_until_successful_write(
            stage="TRANSLATION",
            focus_src=src_rel,
            focus_out=out_rel,
            user_payload=user_payload,
            max_turns=MAX_SESSION_TURNS,
        )
        return ok

    def _repair_file_until_builds(self, *, project_summary: str, src_rel: str, out_rel: str) -> bool:
        """
        Targeted repair loop for a single Lean file. We keep reminding layout and constraints each turn.
        """
        target_mod = module_name_from_lean_path(out_rel)
        if not target_mod:
            log(f"Cannot compute module name for {out_rel}")
            return False

        out = run_lake_build_target(self.spec_pkg_root, target=target_mod)
        if out.startswith("Build Success"):
            return True

        for step in range(MAX_REPAIR_TURNS):
            errs = parse_lean_errors(out, max_n=5)
            p = self.spec_src_root / out_rel
            cur = _read_text_file(p) if p.exists() else ""

            # Prefer first parseable error, but include several
            if errs:
                e0 = errs[0]
                snippet = excerpt_around(cur, e0.line, radius=14)
                err_lines = "\n".join([f"{e.file}:{e.line}:{e.col}: {e.msg}" for e in errs])
            else:
                snippet = trunc(cur, 1200)
                err_lines = trunc(out, 1600)

            src_txt = _read_text_file(self.source_root / src_rel)

            user_payload = (
                "TASK: Fix the target Lean file so that it compiles.\n"
                "You MUST edit ONLY the target Lean file. Do not create new files.\n"
                "You may read other files if necessary, but the final action must be write_lean_file on the target.\n\n"
                "PROJECT SUMMARY:\n"
                f"{project_summary}\n\n"
                f"SOURCE FILE (truth): {src_rel}\n"
                "----- BEGIN SOURCE CONTENT -----\n"
                f"{trunc(src_txt, 6000)}\n"
                "----- END SOURCE CONTENT -----\n\n"
                f"TARGET LEAN FILE: {out_rel}\n"
                "----- BEGIN CURRENT TARGET CONTENT (truncated) -----\n"
                f"{trunc(cur, 7000)}\n"
                "----- END CURRENT TARGET CONTENT -----\n\n"
                "BUILD ERRORS (focus on the FIRST one, make minimal change):\n"
                f"{trunc(err_lines, 2400)}\n\n"
                "LOCAL SNIPPET AROUND FIRST ERROR:\n"
                f"{snippet}\n\n"
                "REPAIR RULES:\n"
                "- Make the smallest change that resolves the first error.\n"
                "- Do not import Std/Mathlib directly; keep import Spec.Prelude.\n"
                f"- Keep namespace Spec.{self.name}.\n"
            )

            ok = self._session_until_successful_write(
                stage=f"REPAIR step {step+1}",
                focus_src=src_rel,
                focus_out=out_rel,
                user_payload=user_payload,
                max_turns=MAX_SESSION_TURNS,
            )
            if not ok:
                log("Repair session stalled.")
                return False

            out = run_lake_build_target(self.spec_pkg_root, target=target_mod)
            log(trunc(out, 1800))
            if out.startswith("Build Success"):
                return True

        return False

    def run_stage_translation(self) -> None:
        log("--- Stage: Translation ---")

        # Summary once
        project_summary = self._project_summary()
        log("Project summary computed.")

        # Translate each source file to its mapped target file, then repair until it builds.
        for src_rel, out_rel in sorted(self.src_to_lean.items()):
            log(f"Translating {src_rel} -> {out_rel}")

            ok = self._translate_file(project_summary=project_summary, src_rel=src_rel, out_rel=out_rel)
            if not ok:
                log(f"CRITICAL: Translation session failed for {src_rel}")
                return

            ok = self._repair_file_until_builds(project_summary=project_summary, src_rel=src_rel, out_rel=out_rel)
            if not ok:
                log(f"CRITICAL: Failed to converge {out_rel} to a compiling state.")
                return

        # Full build check
        out = run_lake_build(self.spec_pkg_root)
        if not out.startswith("Build Success"):
            log("Full build failed after per-file convergence.")
            log(trunc(out, 4000))
            return

        log("Translation complete: lake build succeeded.")

    # ---------------- Equivalence + Spec + Aristotle ----------------

    def run_stage_equivalence(self) -> None:
        """
        Optional: agent creates harnesses + generator and runs differential test.
        Note: Writes are still locked down to pre-autogenerated harness files only.
        """
        log("--- Stage: Equivalence ---")

        # Provide the model with a short brief and require it to write the harness/generator files.
        # It can only write:
        # - spec/tests/gen_inputs.py
        # - spec/tests/harness.c
        # - spec/Spec/tests/Harness.lean
        project_files = sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/")])
        brief = (
            "TASK: Create a differential testing setup.\n"
            "You must implement three files (already autogenerated and writable):\n"
            "- spec/tests/gen_inputs.py\n"
            "- spec/tests/harness.c\n"
            "- tests/Harness.lean (under spec/Spec/)\n\n"
            "Then call run_differential_test with:\n"
            "- gen_script_path = spec/tests/gen_inputs.py\n"
            "- c_harness_path = spec/tests/harness.c\n"
            "- lean_harness_path = tests/Harness.lean\n\n"
            "If there is a diff, fix the translated Lean code (only in allowed Lean files) and rerun.\n"
            "Stop when outputs match, then call submit_stage.\n\n"
            "Translated project Lean files (for context):\n"
            + "\n".join(_limit_lines(project_files, 120))
        )

        instructions = self._base_instructions(stage="EQUIVALENCE")

        previous_response_id: Optional[str] = None
        current_input: Any = brief

        for turn in range(25):
            log(f"Turn {turn+1}")
            resp = self._responses_create(
                instructions=instructions,
                input_data=current_input,
                previous_response_id=previous_response_id,
                tool_choice=None,
                parallel_tool_calls=False,
            )
            previous_response_id = resp.id

            tool_calls = []
            if getattr(resp, "output", None):
                for item in resp.output:
                    if item.type == "message":
                        for part in item.content:
                            if part.type == "output_text":
                                log(f"Model: {trunc(part.text)}")
                    elif item.type == "function_call":
                        tool_calls.append(item)

            if not tool_calls:
                log("No tool calls; exiting equivalence stage.")
                return

            tool_outputs: List[Dict[str, Any]] = []
            for call in tool_calls:
                out_item, _ = self._execute_tool_call(call)
                tool_outputs.append(out_item)

            current_input = tool_outputs

            # If submit_stage was called, stop
            if any(c.name == "submit_stage" for c in tool_calls):
                break

        # Ensure project still builds
        out = run_lake_build(self.spec_pkg_root)
        log(trunc(out, 2000))

    def run_stage_specification(self) -> None:
        log("--- Stage: Specification ---")

        out_rel = f"{self.name}/Verif.lean"
        src_list = "\n".join(_limit_lines(sorted(self.src_to_lean.keys()), 120))
        lean_list = "\n".join(_limit_lines(sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/")]), 160))

        # Provide contents of a few key files to ground it
        sample_files = _limit_lines(sorted([p for p in self.allowed_lean_writes if p.startswith(f"{self.name}/")]), 6)
        sample_blob: List[str] = []
        for rel in sample_files:
            p = self.spec_src_root / rel
            if p.exists():
                sample_blob.append(f"FILE: {rel}\n" + trunc(_read_text_file(p), 2500) + "\n")

        user_payload = (
            "TASK: Write a specification file that states key invariants and theorems for this translated project.\n"
            "You must write ONLY the target file. Proofs may use sorry, but the file must parse and typecheck.\n\n"
            f"TARGET: {out_rel}\n\n"
            "SOURCE FILES:\n"
            f"{src_list}\n\n"
            "TRANSLATED LEAN FILES:\n"
            f"{lean_list}\n\n"
            "SAMPLE LEAN CONTENT:\n"
            + "\n".join(sample_blob)
            + "\nREQUIREMENTS:\n"
              f"- Namespace must be Spec.{self.name}\n"
              "- Must import Spec.Prelude\n"
              "- Include several theorems about safety/consistency and the main operations.\n"
        )

        ok = self._session_until_successful_write(
            stage="SPECIFICATION",
            focus_src="(project)",
            focus_out=out_rel,
            user_payload=user_payload,
            max_turns=MAX_SESSION_TURNS,
        )
        if not ok:
            log("Specification session stalled.")
            return

        # Build to ensure typechecks
        out = run_lake_build(self.spec_pkg_root)
        log(trunc(out, 2000))

    def run_stage_verification(self) -> None:
        target_file = self.spec_project_root / "Verif.lean"
        if not target_file.exists():
            log("No Verif.lean found. Skipping Aristotle.")
            return
        if not aristotlelib:
            log("aristotlelib missing. Skipping Aristotle.")
            return

        log("=== Aristotle Verification ===")
        os.environ["ARISTOTLE_API_KEY"] = self.secrets["secrets"].get("ARISTOTLE_API_KEY", "")

        try:
            cwd = os.getcwd()
            os.chdir(self.spec_pkg_root)

            rel_target = target_file.relative_to(self.spec_pkg_root)
            log(f"Submitting {rel_target} to Aristotle...")

            result = asyncio.run(
                aristotlelib.Project.prove_from_file(
                    input_file_path=str(rel_target),
                    auto_add_imports=True,
                    validate_lean_project=True,
                    wait_for_completion=True,
                )
            )

            os.chdir(cwd)
            log(f"Aristotle Output: {result}")

            res_path = self.spec_pkg_root / result
            if res_path.exists():
                res_path.rename(target_file)
                log("Verified spec saved over Verif.lean.")
                bres = run_lake_build(self.spec_pkg_root)
                log(f"Final Build: {bres}")

        except Exception as e:
            log(f"Aristotle Error: {e}")
            os.chdir(cwd)

    # ---------------- Main runner ----------------

    def run(self) -> None:
        log(f"=== Processing Project: {self.name} ===")
        self._autogen_scaffold_and_lockdown()

        if not self.src_to_lean:
            log("No source mapping; skipping.")
            return

        self.run_stage_translation()
        self.run_stage_equivalence()
        self.run_stage_specification()
        self.run_stage_verification()


# ============================================================
# Main
# ============================================================

def main() -> None:
    log("=== Anneal Universal Verification Agent ===")
    secrets = load_secrets()
    client = OpenAI(api_key=secrets["secrets"]["OPENAI_API_KEY"])

    if not EXAMPLES_DIR.exists():
        log(f"No examples found in {EXAMPLES_DIR}")
        return

    examples = [d for d in EXAMPLES_DIR.iterdir() if d.is_dir()]
    if not examples:
        log("No examples found.")
        return

    for ex in examples:
        proc = ProjectProcessor(ex.name, ex, client, secrets)
        proc.run()


if __name__ == "__main__":
    main()
